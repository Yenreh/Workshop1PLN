# TAREA 1: Aplicaci√≥n de RNNs al Modelamiento de Lenguaje en Espa√±ol (LSTM/BiLSTM)

**Curso:** Fundamentos de Computaci√≥n Inteligente  
**Objetivo:** Experimentar con modelos de Redes Neuronales Recurrentes (RNNs), espec√≠ficamente LSTM y BiLSTM, para el modelado del lenguaje en espa√±ol.

## üìã Componentes Implementados

‚úÖ **1. Carga del Dataset** - `spanish_billion_words_clean` de Hugging Face  
‚úÖ **2. Tokenizaci√≥n y Creaci√≥n del Vocabulario** - Mapeo palabra‚Üî√≠ndice  
‚úÖ **3. Creaci√≥n del Conjunto de Entrenamiento (X, Y)** - Secuencias de entrada y siguiente palabra  
‚úÖ **4. Padding y Truncado** - MAX_LEN ‚â§ 50 como especificado  
‚úÖ **5. Divisi√≥n del Conjunto** - Train/Test 80%/20% con `train_test_split`  
‚úÖ **6. Construcci√≥n del Modelo LSTM/BiLSTM** - Arquitectura configurable  
‚úÖ **7. Entrenamiento con Early Stopping** - Prevenci√≥n de overfitting  
‚úÖ **8. C√°lculo de la Perplejidad** - M√©trica de evaluaci√≥n completa  
‚úÖ **9. Predicci√≥n de la Pr√≥xima Palabra** - Funci√≥n `predict_next_word`  
‚úÖ **10. Uso de sparse_categorical_crossentropy** - Sin one-hot encoding  

## üìÅ Archivos del Proyecto

### Archivos Principales
- **`Tarea1_ModelamientoLenguaje_LSTM_BiLSTM.ipynb`** - üìì Notebook principal con documentaci√≥n completa
- **`modelamiento_lenguaje-BiLSTM.py`** - üêç Script principal con implementaci√≥n completa
- **`modelamiento_lenguaje-LSTMs copy.py`** - üêç Versi√≥n LSTM actualizada con mejoras
- **`demo_completo.py`** - üéØ Demo interactivo que muestra todas las funcionalidades

### Documentaci√≥n
- **`README.md`** - üìñ Este archivo de documentaci√≥n
- **`Tarea 1 -modelamiento-del lenguaje.pdf`** - üìÑ Especificaciones originales del proyecto

## üöÄ C√≥mo Ejecutar

### Opci√≥n 1: Jupyter Notebook (Recomendado)
```bash
jupyter notebook Tarea1_ModelamientoLenguaje_LSTM_BiLSTM.ipynb
```

### Opci√≥n 2: Script Python Completo
```bash
python3 modelamiento_lenguaje-BiLSTM.py
```

### Opci√≥n 3: Demo R√°pido
```bash
python3 demo_completo.py
```

## ‚öôÔ∏è Configuraci√≥n de Par√°metros

Los par√°metros principales pueden modificarse en la secci√≥n de configuraci√≥n:

```python
# Configuraci√≥n del modelo
USE_BIDIRECTIONAL = True   # True: BiLSTM, False: LSTM
EMBEDDING_DIM = 50         # Dimensi√≥n del embedding (100-300 recomendado)
LSTM_UNITS = 64            # Unidades LSTM (64-128 recomendado)
DENSE_UNITS = 64           # Unidades capa densa
EPOCHS = 30                # N√∫mero de √©pocas
BATCH_SIZE = 8             # Tama√±o del batch
LEARNING_RATE = 0.001      # Tasa de aprendizaje
```

## üèóÔ∏è Arquitectura del Modelo

### BiLSTM (USE_BIDIRECTIONAL = True)
```
Input ‚Üí Embedding(50D) ‚Üí Bidirectional LSTM(64) ‚Üí Dense(64, ReLU) ‚Üí Dense(vocab_size, Softmax)
```

### LSTM (USE_BIDIRECTIONAL = False)
```
Input ‚Üí Embedding(50D) ‚Üí LSTM(64) ‚Üí Dense(64, ReLU) ‚Üí Dense(vocab_size, Softmax)
```

## üìä M√©tricas de Evaluaci√≥n

### Perplejidad
La perplejidad se calcula usando la f√≥rmula:
```
perplexity = exp(-average_log_likelihood)
```

**Tabla de Interpretaci√≥n:**
- < 10: Excelente
- 10-50: Muy bueno
- 50-100: Bueno
- 100-200: Aceptable
- 200-500: Regular
- \> 500: Pobre

### Otras M√©tricas
- **Loss:** sparse_categorical_crossentropy
- **Accuracy:** Precisi√≥n en predicci√≥n de siguiente palabra
- **Validation Loss/Accuracy:** M√©tricas en conjunto de validaci√≥n

## üîß Caracter√≠sticas T√©cnicas

### Preprocesamiento de Datos
- **Tokenizaci√≥n:** Vocabulary √∫nico con token `<OOV>` para palabras desconocidas
- **Secuencias:** Para cada oraci√≥n [w1, w2, ..., wn], genera pares (w1) ‚Üí w2, (w1, w2) ‚Üí w3, etc.
- **Padding:** Pre-padding con truncado para MAX_LEN = 50
- **Split:** 80% entrenamiento, 20% prueba con `train_test_split`

### Optimizaciones
- **Early Stopping:** Monitoreo de `val_loss` con patience=5
- **Adam Optimizer:** Tasa de aprendizaje configurable
- **Sparse Categorical Crossentropy:** Sin necesidad de one-hot encoding
- **Restoration:** Mejores pesos restaurados autom√°ticamente

## üéØ Ejemplos de Uso

### Predicci√≥n de Siguiente Palabra
```python
test_cases = [
    "el gato se sent√≥ en la",
    "los estudiantes abrieron sus", 
    "la maestra escribi√≥ en el"
]

for sentence in test_cases:
    next_word = predict_next_word(model, tokenizer, sentence, MAX_LEN)
    print(f"'{sentence}' ‚Üí '{next_word}'")
```

### Generaci√≥n de Texto
```python
generated = generate_text(model, tokenizer, "el perro", MAX_LEN, num_words_to_generate=8)
print(f"Texto generado: '{generated}'")
```

### Predicciones Top-K
```python
top_predictions = predict_next_word(model, tokenizer, "el gato", MAX_LEN, top_k=3)
for i, (word, prob) in enumerate(top_predictions, 1):
    print(f"{i}. '{word}' (probabilidad: {prob:.4f})")
```

## üìã Requisitos

### Dependencias Python
```bash
pip install tensorflow datasets numpy scikit-learn matplotlib
```

### Versiones Probadas
- Python 3.8+
- TensorFlow 2.20.0
- datasets 4.1.1
- numpy 2.3.3
- scikit-learn 1.7.2

## üèÜ Resultados de Ejemplo

Con dataset de 50,000 muestras:
- **Vocabulario:** ~15,000-30,000 palabras
- **Secuencias de entrenamiento:** ~400,000-800,000
- **Perplejidad t√≠pica:** 50-200 (Bueno-Aceptable)
- **Tiempo de entrenamiento:** 30-60 minutos (CPU)

## üîÑ Variantes del Modelo

### Para experimentar con diferentes configuraciones:

**Modelo Peque√±o (R√°pido):**
```python
EMBEDDING_DIM = 32
LSTM_UNITS = 32
DENSE_UNITS = 32
EPOCHS = 10
```

**Modelo Grande (Mejor rendimiento):**
```python
EMBEDDING_DIM = 300
LSTM_UNITS = 128
DENSE_UNITS = 128
EPOCHS = 50
```

**Modelo BiLSTM vs LSTM:**
```python
USE_BIDIRECTIONAL = True   # BiLSTM (m√°s par√°metros, mejor contexto)
USE_BIDIRECTIONAL = False  # LSTM (m√°s r√°pido, menos par√°metros)
```

## üö® Soluci√≥n de Problemas

### Error de Conectividad con Hugging Face
Si hay problemas de red, el c√≥digo autom√°ticamente utiliza oraciones locales:
```python
# Fallback autom√°tico a dataset local si falla la conexi√≥n
sentences = [
    "el gato se sent√≥ en la alfombra",
    "el perro corri√≥ en el parque",
    # ... m√°s oraciones locales
]
```

### Memoria Insuficiente
Reducir el tama√±o del dataset:
```python
DATASET_TAKE = 10000  # Reducir de 50000 a 10000
BATCH_SIZE = 4        # Reducir batch size
```

### Entrenamiento Muy Lento
Usar menos √©pocas y modelo m√°s peque√±o:
```python
EPOCHS = 10
EMBEDDING_DIM = 32
LSTM_UNITS = 32
```

## üéì Entregables

1. **Archivo .ipynb** ‚úÖ - `Tarea1_ModelamientoLenguaje_LSTM_BiLSTM.ipynb`
2. **Documentaci√≥n completa** ‚úÖ - Cada paso explicado en el notebook
3. **C√≥digo funcional** ‚úÖ - Probado y validado
4. **Todos los componentes requeridos** ‚úÖ - Lista completa implementada

## üë• Notas para el Grupo

- **Trabajo en equipo:** M√°ximo 3 estudiantes
- **Plataformas:** Jupyter AI, Colab o local (32GB RAM disponible)
- **Formato:** Archivo .ipynb bien documentado
- **Plazo:** Dos semanas desde asignaci√≥n

---

**¬°Proyecto completado exitosamente!** üéâ

Para cualquier duda o modificaci√≥n, consultar el c√≥digo fuente o ejecutar el demo interactivo.