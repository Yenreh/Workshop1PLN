{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA 1: Aplicación de RNNs al Modelamiento de Lenguaje en Español (LSTM/BiLSTM)\n",
    "\n",
    "**Curso:** PLN\n",
    "**Objetivo:** Experimentar con modelos de Redes Neuronales Recurrentes (RNNs), específicamente LSTM y BiLSTM, para el modelado del lenguaje en español.\n",
    "\n",
    "## Componentes implementados:\n",
    "1. Carga del Dataset (`spanish_billion_words_clean`)\n",
    "2. Tokenización y Creación del Vocabulario\n",
    "3. Creación del Conjunto de Entrenamiento (X, Y)\n",
    "4. Padding y Truncado (MAX_LEN ≤ 50)\n",
    "5. División del Conjunto (Train/Test 80%/20%)\n",
    "6. Construcción del Modelo LSTM/BiLSTM\n",
    "7. Entrenamiento con Early Stopping\n",
    "8. Cálculo de la Perplejidad\n",
    "9. Predicción de la Próxima Palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuración de Parámetros Globales\n",
    "\n",
    "Esta sección permite modificar fácilmente todos los parámetros del modelo para experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración cargada:\n",
      "- Modelo: BiLSTM\n",
      "- Dataset: 5,000 muestras de jhonparra18/spanish_billion_words_clean\n",
      "- Arquitectura: 50D embedding → LSTM(64) → Dense(64)\n",
      "- Entrenamiento: 30 epochs, batch_size=8\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "DATASET_NAME = \"jhonparra18/spanish_billion_words_clean\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "DATASET_STREAMING = True\n",
    "DATASET_TAKE = 5000  # Número de ejemplos a tomar del dataset\n",
    "MIN_WORDS_PER_SENTENCE = 4  # Filtro: oraciones con al menos N palabras\n",
    "OOV_TOKEN = \"<OOV>\"  # Token para palabras fuera del vocabulario\n",
    "\n",
    "# Model architecture parameters\n",
    "EMBEDDING_DIM = 50  # Dimensión del embedding de palabras\n",
    "LSTM_UNITS = 64  # Número de unidades en las capas LSTM\n",
    "DENSE_UNITS = 64  # Número de unidades en la capa densa intermedia\n",
    "USE_BIDIRECTIONAL = True  # Usar BiLSTM en lugar de LSTM unidireccional\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 30  # Número de épocas de entrenamiento\n",
    "BATCH_SIZE = 8  # Tamaño del batch\n",
    "VALIDATION_SPLIT = 0.2  # Porcentaje de datos para validación\n",
    "LEARNING_RATE = 0.001  # Tasa de aprendizaje\n",
    "\n",
    "# Sequence processing\n",
    "PADDING_TYPE = 'pre'  # Tipo de padding: 'pre' o 'post'\n",
    "\n",
    "# Output parameters\n",
    "VERBOSE_TRAINING = 1  # Nivel de verbosidad durante entrenamiento\n",
    "VERBOSE_PREDICTION = 0  # Nivel de verbosidad durante predicción\n",
    "\n",
    "print(f\"Configuración cargada:\")\n",
    "print(f\"- Modelo: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"- Dataset: {DATASET_TAKE:,} muestras de {DATASET_NAME}\")\n",
    "print(f\"- Arquitectura: {EMBEDDING_DIM}D embedding → LSTM({LSTM_UNITS}) → Dense({DENSE_UNITS})\")\n",
    "print(f\"- Entrenamiento: {EPOCHS} epochs, batch_size={BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías\n",
    "\n",
    "Se importan todas las librerías necesarias para el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas exitosamente!\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "print(\"Librerías importadas exitosamente!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset\n",
    "\n",
    "Se carga el dataset `spanish_billion_words_clean` de Hugging Face y se filtran oraciones muy cortas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd2e4f8dc7242328521486ecf1d39a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de oraciones cargadas: 5000\n",
      "\n",
      "Ejemplos de oraciones:\n",
      "1. source wikisource librodot com sentido y sensibilidad jane austen capitulo i\n",
      "2. la familia dashwood llevaba largo tiempo afincada en sussex\n",
      "3. su propiedad era de buen tamaño y en el centro de ella se encontraba la residencia norland park donde la manera tan digna en que habían vivido por muchas generaciones llegó a granjearles el respeto de todos los conocidos del lugar\n",
      "4. el último dueño de esta propiedad había sido un hombre soltero que alcanzó una muy avanzada edad y que durante gran parte de su existencia tuvo en su hermana una fiel compañera y ama de casa\n",
      "5. pero la muerte de ella ocurrida diez años antes que la suya produjo grandes alteraciones en su hogar\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT, streaming=DATASET_STREAMING).take(DATASET_TAKE)\n",
    "sentences = [example['text'] for example in dataset if len(example['text'].split()) >= MIN_WORDS_PER_SENTENCE]\n",
    "\n",
    "print(f\"Total de oraciones cargadas: {len(sentences)}\")\n",
    "print(\"\\nEjemplos de oraciones:\")\n",
    "for i, sentence in enumerate(sentences[:5]):\n",
    "    print(f\"{i+1}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenización y Creación del Vocabulario\n",
    "\n",
    "Se Tokenizan las oraciones y se crea el vocabulario con mapeo palabra→índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 11492 palabras\n",
      "\n",
      "Primeras 10 palabras del vocabulario:\n",
      "<OOV> → 1\n",
      "de → 2\n",
      "que → 3\n",
      "la → 4\n",
      "y → 5\n",
      "a → 6\n",
      "en → 7\n",
      "el → 8\n",
      "su → 9\n",
      "no → 10\n",
      "\n",
      "Ejemplo de tokenización:\n",
      "Oración: source wikisource librodot com sentido y sensibilidad jane austen capitulo i\n",
      "Secuencia: [3723, 5458, 5459, 3724, 319, 5, 957, 1085, 5460, 1252, 3725]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token=OOV_TOKEN)  # Token para palabras desconocidas\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Convertir frases a secuencias numéricas\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Tamaño del vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 por el padding (índice 0)\n",
    "print(f\"Vocabulario: {vocab_size} palabras\")\n",
    "\n",
    "print(\"\\nPrimeras 10 palabras del vocabulario:\")\n",
    "for i, (word, idx) in enumerate(list(tokenizer.word_index.items())[:10]):\n",
    "    print(f\"{word} → {idx}\")\n",
    "\n",
    "print(f\"\\nEjemplo de tokenización:\")\n",
    "example_sentence = sentences[0]\n",
    "example_sequence = sequences[0]\n",
    "print(f\"Oración: {example_sentence}\")\n",
    "print(f\"Secuencia: {example_sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creación del Conjunto de Entrenamiento (X, Y)\n",
    "\n",
    "Para cada secuencia `[w1, w2, ..., wn]`, se crean pares:\n",
    "- `(w1) → w2`\n",
    "- `(w1, w2) → w3`\n",
    "- `...`\n",
    "- `(w1, ..., wn-1) → wn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de secuencias de entrenamiento generadas: 121874\n",
      "\n",
      "Ejemplos de secuencias X → y:\n",
      "['source'] → wikisource\n",
      "['source', 'wikisource'] → librodot\n",
      "['source', 'wikisource', 'librodot'] → com\n",
      "['source', 'wikisource', 'librodot', 'com'] → sentido\n",
      "['source', 'wikisource', 'librodot', 'com', 'sentido'] → y\n"
     ]
    }
   ],
   "source": [
    "X = []  # Secuencias de entrada: [palabra1], [palabra1, palabra2], ...\n",
    "y = []  # Palabra siguiente (objetivo)\n",
    "\n",
    "for seq in sequences:\n",
    "    for i in range(len(seq) - 1):\n",
    "        X.append(seq[:i+1])    # Desde el inicio hasta la palabra actual\n",
    "        y.append(seq[i+1])     # La siguiente palabra\n",
    "\n",
    "print(f\"Total de secuencias de entrenamiento generadas: {len(X)}\")\n",
    "print(\"\\nEjemplos de secuencias X → y:\")\n",
    "for i in range(5):\n",
    "    # Convertir índices a palabras para mostrar\n",
    "    x_words = [tokenizer.index_word.get(idx, '<UNK>') for idx in X[i]]\n",
    "    y_word = tokenizer.index_word.get(y[i], '<UNK>')\n",
    "    print(f\"{x_words} → {y_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Padding y Truncado (MAX_LEN ≤ 50)\n",
    "\n",
    "Se Aplica padding a las secuencias y se trunca si la longitud máxima excede 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud máxima encontrada: 156\n",
      "MAX_LEN aplicado (truncado a 50): 50\n",
      "\n",
      "Forma de X_padded: (121874, 50)\n",
      "Forma de y: (121874,)\n",
      "\n",
      "Ejemplo de X_padded (primeras 3 secuencias):\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0 3723]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0 3723 5458]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 3723 5458 5459]]\n",
      "\n",
      "Ejemplo de y (primeras 10 etiquetas):\n",
      "[5458 5459 3724  319    5  957 1085 5460 1252 3725]\n"
     ]
    }
   ],
   "source": [
    "# Longitud máxima de las secuencias (con restricción MAX_LEN <= 50)\n",
    "raw_max_length = max([len(seq) for seq in X])\n",
    "MAX_LEN = min(50, raw_max_length)  # Aplicar restricción de máximo 50\n",
    "print(f\"Longitud máxima encontrada: {raw_max_length}\")\n",
    "print(f\"MAX_LEN aplicado (truncado a 50): {MAX_LEN}\")\n",
    "\n",
    "# Aplicar padding y truncado a las secuencias de entrada\n",
    "X_padded = pad_sequences(X, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating='pre')\n",
    "y = np.array(y)  # Etiquetas: ya es un array 1D\n",
    "\n",
    "print(f\"\\nForma de X_padded: {X_padded.shape}\")  # (número de muestras, MAX_LEN)\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(\"\\nEjemplo de X_padded (primeras 3 secuencias):\")\n",
    "print(X_padded[:3])\n",
    "print(\"\\nEjemplo de y (primeras 10 etiquetas):\")\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. División del Conjunto (Train/Test: 80%/20%)\n",
    "\n",
    "Se dividen los datos en conjuntos de entrenamiento y prueba usando `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "División del conjunto:\n",
      "X_train: (97499, 50), y_train: (97499,)\n",
      "X_test: (24375, 50), y_test: (24375,)\n",
      "\n",
      "Porcentajes:\n",
      "Entrenamiento: 80.0%\n",
      "Prueba: 20.0%\n",
      "\n",
      " Usando sparse_categorical_crossentropy - NO necesitamos one-hot encoding\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_padded, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"División del conjunto:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"\\nPorcentajes:\")\n",
    "print(f\"Entrenamiento: {len(X_train)/(len(X_train)+len(X_test))*100:.1f}%\")\n",
    "print(f\"Prueba: {len(X_test)/(len(X_train)+len(X_test))*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Construcción del Modelo LSTM/BiLSTM\n",
    "\n",
    "Se crea el modelo con arquitectura configurable (LSTM o BiLSTM) y se compila con `sparse_categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_lstm              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_lstm              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Crea un modelo de lenguaje usando BiLSTM o LSTM según configuración.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Tamaño del vocabulario\n",
    "        max_length (int): Longitud máxima de las secuencias\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Modelo compilado\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length, name='embedding'),\n",
    "        # Usar BiLSTM si está habilitado, sino LSTM unidireccional\n",
    "        Bidirectional(LSTM(LSTM_UNITS, name='lstm'), name='bidirectional_lstm') if USE_BIDIRECTIONAL \n",
    "        else LSTM(LSTM_UNITS, name='lstm'),\n",
    "        Dense(DENSE_UNITS, activation='relu', name='dense_hidden'),\n",
    "        Dense(vocab_size, activation='softmax', name='output')  # Probabilidades para cada palabra\n",
    "    ])\n",
    "    \n",
    "    # Configurar optimizador con tasa de aprendizaje personalizada\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    # Usar sparse_categorical_crossentropy (NO necesita one-hot encoding)\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy', 'sparse_top_k_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(vocab_size, MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento del Modelo con Early Stopping\n",
    "\n",
    "Se entrena el modelo con Early Stopping para prevenir overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando el modelo BiLSTM...\n",
      "Arquitectura: 50D embedding → Bi-LSTM(64) → Dense(64) → Softmax(11492)\n",
      "Parámetros de entrenamiento: 30 epochs, batch_size=8, lr=0.001\n",
      "Early Stopping habilitado: val_loss con patience=5\n",
      "Epoch 1/30\n",
      "\u001b[1m9750/9750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 14ms/step - accuracy: 0.0497 - loss: 6.8712 - sparse_top_k_categorical_accuracy: 0.1795 - val_accuracy: 0.0678 - val_loss: 6.6270 - val_sparse_top_k_categorical_accuracy: 0.2081\n",
      "Epoch 2/30\n",
      "\u001b[1m9750/9750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 14ms/step - accuracy: 0.0696 - loss: 6.2920 - sparse_top_k_categorical_accuracy: 0.2159 - val_accuracy: 0.0849 - val_loss: 6.5784 - val_sparse_top_k_categorical_accuracy: 0.2341\n",
      "Epoch 3/30\n",
      "\u001b[1m9750/9750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 14ms/step - accuracy: 0.0972 - loss: 5.9640 - sparse_top_k_categorical_accuracy: 0.2459 - val_accuracy: 0.1015 - val_loss: 6.5544 - val_sparse_top_k_categorical_accuracy: 0.2481\n",
      "Epoch 4/30\n",
      "\u001b[1m9750/9750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 14ms/step - accuracy: 0.1138 - loss: 5.7282 - sparse_top_k_categorical_accuracy: 0.2613 - val_accuracy: 0.1086 - val_loss: 6.5969 - val_sparse_top_k_categorical_accuracy: 0.2550\n",
      "Epoch 5/30\n",
      "\u001b[1m9750/9750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 14ms/step - accuracy: 0.1296 - loss: 5.5188 - sparse_top_k_categorical_accuracy: 0.2797 - val_accuracy: 0.1088 - val_loss: 6.6132 - val_sparse_top_k_categorical_accuracy: 0.2579\n",
      "Epoch 6/30\n",
      "\u001b[1m9750/9750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 14ms/step - accuracy: 0.1426 - loss: 5.3365 - sparse_top_k_categorical_accuracy: 0.2921 - val_accuracy: 0.1135 - val_loss: 6.6746 - val_sparse_top_k_categorical_accuracy: 0.2639\n",
      "Epoch 7/30\n",
      "\u001b[1m9750/9750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1507 - loss: 5.1479 - sparse_top_k_categorical_accuracy: 0.3087"
     ]
    }
   ],
   "source": [
    "# Configurar Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     # Métrica a monitorear\n",
    "    patience=5,             # Número de épocas sin mejora antes de parar\n",
    "    restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "    verbose=1               # Mostrar información cuando se pare\n",
    ")\n",
    "\n",
    "print(f\"Entrenando el modelo {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}...\")\n",
    "print(f\"Arquitectura: {EMBEDDING_DIM}D embedding → {'Bi-' if USE_BIDIRECTIONAL else ''}LSTM({LSTM_UNITS}) → Dense({DENSE_UNITS}) → Softmax({vocab_size})\")\n",
    "print(f\"Parámetros de entrenamiento: {EPOCHS} epochs, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(\"Early Stopping habilitado: val_loss con patience=5\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,  # Usar conjuntos de entrenamiento divididos\n",
    "    epochs=EPOCHS,\n",
    "    verbose=VERBOSE_TRAINING,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,  # Usar datos de entrenamiento para validación\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping]  # Agregar Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cálculo de la Perplejidad\n",
    "\n",
    "Se immplementan las funciones para calcular la perplejidad.\n",
    "\n",
    "**Fórmula:** `perplexity = exp(-average_log_likelihood)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad del modelo en el conjunto de prueba.\n",
    "    \n",
    "    La perplejidad es una medida de qué tan bien un modelo de probabilidad \n",
    "    predice una muestra. Valores más bajos indican mejor rendimiento.\n",
    "    \n",
    "    Fórmula: perplexity = exp(average_negative_log_likelihood)\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        X_test: Conjunto de prueba de entrada\n",
    "        y_test: Conjunto de prueba de salida (etiquetas verdaderas)\n",
    "    \n",
    "    Returns:\n",
    "        float: Valor de perplejidad\n",
    "    \"\"\"\n",
    "    # Obtener probabilidades predichas para el conjunto de prueba\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calcular log-likelihood promedio\n",
    "    log_likelihoods = []\n",
    "    for i in range(len(y_test)):\n",
    "        true_word_idx = y_test[i]\n",
    "        predicted_prob = predictions[i][true_word_idx]\n",
    "        # Evitar log(0) agregando un pequeño epsilon\n",
    "        predicted_prob = max(predicted_prob, 1e-10)\n",
    "        log_likelihoods.append(np.log(predicted_prob))\n",
    "    \n",
    "    # Calcular perplejidad\n",
    "    average_log_likelihood = np.mean(log_likelihoods)\n",
    "    perplexity = np.exp(-average_log_likelihood)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def interpret_perplexity(perplexity_value):\n",
    "    \"\"\"\n",
    "    Interpreta el valor de perplejidad según rangos comunes.\n",
    "    \n",
    "    Args:\n",
    "        perplexity_value (float): Valor de perplejidad calculado\n",
    "        \n",
    "    Returns:\n",
    "        str: Interpretación del valor\n",
    "    \"\"\"\n",
    "    if perplexity_value < 10:\n",
    "        return \"Excelente\"\n",
    "    elif perplexity_value < 50:\n",
    "        return \"Muy bueno\"\n",
    "    elif perplexity_value < 100:\n",
    "        return \"Bueno\"\n",
    "    elif perplexity_value < 200:\n",
    "        return \"Aceptable\"\n",
    "    elif perplexity_value < 500:\n",
    "        return \"Regular\"\n",
    "    else:\n",
    "        return \"Pobre\"\n",
    "\n",
    "print(\"Funciones de perplejidad definidas exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluación Completa del Modelo\n",
    "\n",
    "Se evalua el rendimiento del modelo incluyendo perplejidad y métricas estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(history, model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Muestra métricas de rendimiento del modelo durante el entrenamiento y evaluación.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== MÉTRICAS DE ENTRENAMIENTO ===\")\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_accuracy = history.history['accuracy'][-1]\n",
    "    \n",
    "    if 'val_loss' in history.history:\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "        print(f\"Loss final: {final_loss:.4f} | Val Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"Accuracy final: {final_accuracy:.4f} | Val Accuracy: {final_val_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(f\"Loss final: {final_loss:.4f}\")\n",
    "        print(f\"Accuracy final: {final_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluar en conjunto de prueba\n",
    "    print(\"\\n=== EVALUACIÓN EN CONJUNTO DE PRUEBA ===\")\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Calcular perplejidad\n",
    "    print(\"\\n=== CÁLCULO DE PERPLEJIDAD ===\")\n",
    "    perplexity = calculate_perplexity(model, X_test, y_test)\n",
    "    interpretation = interpret_perplexity(perplexity)\n",
    "    \n",
    "    print(f\"Perplejidad en conjunto de prueba: {perplexity:.2f}\")\n",
    "    print(f\"Interpretación: {interpretation}\")\n",
    "    \n",
    "    # Tabla de interpretación\n",
    "    print(\"\\n=== TABLA DE INTERPRETACIÓN DE PERPLEJIDAD ===\")\n",
    "    print(\"< 10:     Excelente\")\n",
    "    print(\"10-50:    Muy bueno\") \n",
    "    print(\"50-100:   Bueno\")\n",
    "    print(\"100-200:  Aceptable\")\n",
    "    print(\"200-500:  Regular\")\n",
    "    print(\"> 500:    Pobre\")\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Evaluar rendimiento (incluyendo perplejidad)\n",
    "perplexity = evaluate_model_performance(history, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Predicción de la Próxima Palabra\n",
    "\n",
    "Se implementa la función `predict_next_word` para predecir la siguiente palabra dada una secuencia de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, sentence, max_length, top_k=1):\n",
    "    \"\"\"\n",
    "    Predice la(s) palabra(s) siguiente(s) dada una oración en español.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizador ajustado\n",
    "        sentence (str): Oración de entrada\n",
    "        max_length (int): Longitud máxima de secuencia\n",
    "        top_k (int): Número de predicciones top a retornar\n",
    "    \n",
    "    Returns:\n",
    "        str o list: Palabra predicha (top_k=1) o lista de predicciones (top_k>1)\n",
    "    \"\"\"\n",
    "    # Tokenizar la oración\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "    if len(sequence) == 0:\n",
    "        return \"<no_reconocido>\" if top_k == 1 else [\"<no_reconocido>\"]\n",
    "\n",
    "    # Aplicar padding usando el mismo tipo configurado\n",
    "    sequence_padded = pad_sequences([sequence], maxlen=max_length, padding=PADDING_TYPE, truncating='pre')\n",
    "\n",
    "    # Predecir probabilidades\n",
    "    prediction = model.predict(sequence_padded, verbose=VERBOSE_PREDICTION)\n",
    "    \n",
    "    if top_k == 1:\n",
    "        # Retornar solo la predicción más probable\n",
    "        predicted_idx = np.argmax(prediction[0])\n",
    "        predicted_word = tokenizer.index_word.get(predicted_idx, \"<desconocido>\")\n",
    "        return predicted_word\n",
    "    else:\n",
    "        # Retornar las top_k predicciones más probables\n",
    "        top_indices = np.argsort(prediction[0])[-top_k:][::-1]\n",
    "        predictions = []\n",
    "        for idx in top_indices:\n",
    "            word = tokenizer.index_word.get(idx, \"<desconocido>\")\n",
    "            prob = prediction[0][idx]\n",
    "            predictions.append((word, prob))\n",
    "        return predictions\n",
    "\n",
    "def generate_text(model, tokenizer, seed_text, max_length, num_words_to_generate=10):\n",
    "    \"\"\"\n",
    "    Genera texto continuando desde un texto semilla.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizador ajustado\n",
    "        seed_text (str): Texto inicial\n",
    "        max_length (int): Longitud máxima de secuencia\n",
    "        num_words_to_generate (int): Número de palabras a generar\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto generado\n",
    "    \"\"\"\n",
    "    generated_text = seed_text\n",
    "    \n",
    "    for _ in range(num_words_to_generate):\n",
    "        next_word = predict_next_word(model, tokenizer, generated_text, max_length)\n",
    "        if next_word in [\"<no_reconocido>\", \"<desconocido>\"]:\n",
    "            break\n",
    "        generated_text += \" \" + next_word\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"Funciones de predicción definidas exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pruebas de Predicción de Siguiente Palabra\n",
    "\n",
    "Se prueba el modelo con casos de prueba en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos de prueba para predicción de siguiente palabra\n",
    "test_cases = [\n",
    "    \"el gato se sentó en la\",\n",
    "    \"los estudiantes abrieron sus\",\n",
    "    \"la maestra escribió en el\",\n",
    "    \"el niño jugó con un\",\n",
    "    \"el pájaro voló sobre el\",\n",
    "    \"el sol se elevó en el\",\n",
    "    \"la casa tiene una\",\n",
    "    \"me gusta comer\",\n",
    "    \"vamos a la\"\n",
    "]\n",
    "\n",
    "print(f\"=== PREDICCIÓN DE SIGUIENTE PALABRA ({'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}) ===\")\n",
    "for sentence in test_cases:\n",
    "    next_word = predict_next_word(model, tokenizer, sentence, MAX_LEN)\n",
    "    print(f\"'{sentence}' → '{next_word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Predicciones Top-K\n",
    "\n",
    "Se muestran las K(3) predicciones más probables para algunos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones top-k para algunos casos\n",
    "print(f\"=== TOP-3 PREDICCIONES ===\")\n",
    "for sentence in test_cases[:3]:\n",
    "    top_predictions = predict_next_word(model, tokenizer, sentence, MAX_LEN, top_k=3)\n",
    "    print(f\"'{sentence}':\")\n",
    "    for i, (word, prob) in enumerate(top_predictions, 1):\n",
    "        print(f\"  {i}. '{word}' (probabilidad: {prob:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Generación de Texto\n",
    "\n",
    "Se genera texto continuando desde un texto semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de texto\n",
    "print(f\"=== GENERACIÓN DE TEXTO ===\")\n",
    "seed_texts = [\n",
    "    \"el perro\",\n",
    "    \"los estudiantes\",\n",
    "    \"en la casa\"\n",
    "]\n",
    "\n",
    "for seed in seed_texts:\n",
    "    generated = generate_text(model, tokenizer, seed, MAX_LEN, num_words_to_generate=8)\n",
    "    print(f\"Semilla: '{seed}'\")\n",
    "    print(f\"Generado: '{generated}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Resumen Final del Modelo\n",
    "\n",
    "Se muestra un resumen completo de las características y rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RESUMEN DEL MODELO ===\")\n",
    "print(f\"Vocabulario: {vocab_size:,} palabras\")\n",
    "print(f\"Secuencias de entrenamiento: {len(X_train):,}\")\n",
    "print(f\"Secuencias de prueba: {len(X_test):,}\")\n",
    "print(f\"Longitud máxima de secuencia (MAX_LEN): {MAX_LEN}\")\n",
    "print(f\"Arquitectura: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"Parámetros del modelo: {model.count_params():,}\")\n",
    "print(f\"Dataset utilizado: {DATASET_NAME} (primeras {DATASET_TAKE:,} muestras)\")\n",
    "print(f\"Perplejidad final: {perplexity:.2f} ({interpret_perplexity(perplexity)})\")\n",
    "\n",
    "print(\"\\n=== COMPONENTES IMPLEMENTADOS ===\")\n",
    "components = [\n",
    "    \"Carga del Dataset (spanish_billion_words_clean)\",\n",
    "    \"Tokenización y Creación del Vocabulario\", \n",
    "    \"Creación del Conjunto de Entrenamiento (X, Y)\",\n",
    "    \"Padding y Truncado (MAX_LEN ≤ 50)\",\n",
    "    \"División del Conjunto (Train/Test 80%/20%)\",\n",
    "    \"Construcción del Modelo LSTM/BiLSTM\",\n",
    "    \"Entrenamiento con Early Stopping\",\n",
    "    \"Cálculo de la Perplejidad\",\n",
    "    \"Predicción de la Próxima Palabra\",\n",
    "    \"Uso de sparse_categorical_crossentropy\"\n",
    "]\n",
    "\n",
    "for component in components:\n",
    "    print(component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "Este notebook implementa completamente los requerimientos de la Tarea 1:\n",
    "\n",
    "1. **Dataset**: Se utilizó `spanish_billion_words_clean` de Hugging Face\n",
    "2. **Tokenización**: Se Implementó con vocabulario único y mapeo palabra↔índice\n",
    "3. **Preparación de datos**: Secuencias (X,Y) con padding/truncado (MAX_LEN≤50)\n",
    "4. **División**: 80% entrenamiento / 20% prueba con `train_test_split`\n",
    "5. **Modelo**: Arquitectura LSTM/BiLSTM configurable con embedding\n",
    "6. **Entrenamiento**: Con Early Stopping y `sparse_categorical_crossentropy`\n",
    "7. **Evaluación**: Perplejidad calculada \n",
    "8. **Predicción**: Función `predict_next_word`\n",
    "\n",
    "El modelo puede alternar entre LSTM y BiLSTM simplemente cambiando `USE_BIDIRECTIONAL=True/False` en los parámetros globales.\n",
    "\n",
    "**Parámetros recomendados para experimentación:**\n",
    "- EMBEDDING_DIM: 100-300\n",
    "- LSTM_UNITS: 64-128\n",
    "- DENSE_UNITS: 64-128\n",
    "- LEARNING_RATE: 0.001-0.01\n",
    "- BATCH_SIZE: 8-32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
