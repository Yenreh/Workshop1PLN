{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 1: AplicaciÃ³n de RNNs al Modelamiento de Lenguaje en EspaÃ±ol (LSTM/BiLSTM)\n",
    "\n",
    "**Curso:** PLN\n",
    "**Objetivo:** Experimentar con modelos de Redes Neuronales Recurrentes (RNNs), especÃ­ficamente LSTM y BiLSTM, para el modelado del lenguaje en espaÃ±ol. \n",
    "**Autor:** Herney Eduardo Quintero Trochez  \n",
    "**Fecha:** 2025  \n",
    "**Universidad:** Universidad Del Valle  \n",
    "**Curso:** Procesamiento de Lenguaje Natural (PLN) - Taller 1\n",
    "## Componentes implementados:\n",
    "1. Carga del Dataset (`spanish_billion_words_clean`)\n",
    "2. TokenizaciÃ³n y CreaciÃ³n del Vocabulario\n",
    "3. CreaciÃ³n del Conjunto de Entrenamiento (X, Y)\n",
    "4. Padding y Truncado (MAX_LEN â‰¤ 50)\n",
    "5. DivisiÃ³n del Conjunto (Train/Test 80%/20%)\n",
    "6. ConstrucciÃ³n del Modelo LSTM/BiLSTM\n",
    "7. Entrenamiento con Early Stopping\n",
    "8. CÃ¡lculo de la Perplejidad\n",
    "9. PredicciÃ³n de la PrÃ³xima Palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ConfiguraciÃ³n de ParÃ¡metros Globales\n",
    "\n",
    "Esta secciÃ³n permite modificar fÃ¡cilmente todos los parÃ¡metros del modelo para experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfiguraciÃ³n cargada:\n",
      "- Modelo: LSTM\n",
      "- Dataset: 20,000 muestras de jhonparra18/spanish_billion_words_clean\n",
      "- Arquitectura: 300D embedding â†’ LSTM(128) â†’ Dense(128)\n",
      "- Entrenamiento: 30 epochs, batch_size=32\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "DATASET_NAME = \"jhonparra18/spanish_billion_words_clean\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "DATASET_STREAMING = True\n",
    "DATASET_TAKE = 20000  # NÃºmero de ejemplos a tomar del dataset\n",
    "MIN_WORDS_PER_SENTENCE = 4  # Filtro: oraciones con al menos N palabras\n",
    "OOV_TOKEN = \"<OOV>\"  # Token para palabras fuera del vocabulario\n",
    "\n",
    "# Model architecture parameters\n",
    "EMBEDDING_DIM = 300  # DimensiÃ³n del embedding de palabras\n",
    "LSTM_UNITS = 128 # NÃºmero de unidades en las capas LSTM\n",
    "DENSE_UNITS = 128  # NÃºmero de unidades en la capa densa intermedia\n",
    "USE_BIDIRECTIONAL = False  # Usar BiLSTM en lugar de LSTM unidireccional\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 30  # NÃºmero de Ã©pocas de entrenamiento\n",
    "BATCH_SIZE = 32  # TamaÃ±o del batch\n",
    "VALIDATION_SPLIT = 0.2  # Porcentaje de datos para validaciÃ³n\n",
    "LEARNING_RATE = 0.001  # Tasa de aprendizaje\n",
    "\n",
    "# Sequence processing\n",
    "PADDING_TYPE = 'pre'  # Tipo de padding: 'pre' o 'post'\n",
    "\n",
    "# Output parameters\n",
    "VERBOSE_TRAINING = 1  # Nivel de verbose durante entrenamiento\n",
    "VERBOSE_PREDICTION = 0  # Nivel de verbose durante predicciÃ³n\n",
    "\n",
    "print(f\"ConfiguraciÃ³n cargada:\")\n",
    "print(f\"- Modelo: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"- Dataset: {DATASET_TAKE:,} muestras de {DATASET_NAME}\")\n",
    "print(f\"- Arquitectura: {EMBEDDING_DIM}D embedding â†’ LSTM({LSTM_UNITS}) â†’ Dense({DENSE_UNITS})\")\n",
    "print(f\"- Entrenamiento: {EPOCHS} epochs, batch_size={BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ImportaciÃ³n de LibrerÃ­as\n",
    "\n",
    "Se importan todas las librerÃ­as necesarias para el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 19:31:59.794475: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-25 19:31:59.836882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-25 19:32:00.785344: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/yenreh/anaconda3/envs/py310ia/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LibrerÃ­as importadas exitosamente!\n",
      "TensorFlow version: 2.20.0\n",
      "GPU detectada: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "print(\"LibrerÃ­as importadas exitosamente!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU detectada: {gpus}\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No se detectÃ³ GPU, usando CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset\n",
    "\n",
    "Se carga el dataset `spanish_billion_words_clean` de Hugging Face y se filtran oraciones muy cortas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset...\n",
      "Total de oraciones cargadas: 20000\n",
      "\n",
      "Ejemplos de oraciones:\n",
      "1. source wikisource librodot com sentido y sensibilidad jane austen capitulo i\n",
      "2. la familia dashwood llevaba largo tiempo afincada en sussex\n",
      "3. su propiedad era de buen tamaÃ±o y en el centro de ella se encontraba la residencia norland park donde la manera tan digna en que habÃ­an vivido por muchas generaciones llegÃ³ a granjearles el respeto de todos los conocidos del lugar\n",
      "4. el Ãºltimo dueÃ±o de esta propiedad habÃ­a sido un hombre soltero que alcanzÃ³ una muy avanzada edad y que durante gran parte de su existencia tuvo en su hermana una fiel compaÃ±era y ama de casa\n",
      "5. pero la muerte de ella ocurrida diez aÃ±os antes que la suya produjo grandes alteraciones en su hogar\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT, streaming=DATASET_STREAMING).take(DATASET_TAKE)\n",
    "sentences = [example['text'] for example in dataset if len(example['text'].split()) >= MIN_WORDS_PER_SENTENCE]\n",
    "\n",
    "print(f\"Total de oraciones cargadas: {len(sentences)}\")\n",
    "print(\"\\nEjemplos de oraciones:\")\n",
    "for i, sentence in enumerate(sentences[:5]):\n",
    "    print(f\"{i+1}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TokenizaciÃ³n y CreaciÃ³n del Vocabulario\n",
    "\n",
    "Se Tokenizan las oraciones y se crea el vocabulario con mapeo palabraâ†’Ã­ndice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 30415 palabras\n",
      "\n",
      "Primeras 10 palabras del vocabulario:\n",
      "<OOV> â†’ 1\n",
      "que â†’ 2\n",
      "de â†’ 3\n",
      "y â†’ 4\n",
      "la â†’ 5\n",
      "a â†’ 6\n",
      "el â†’ 7\n",
      "en â†’ 8\n",
      "no â†’ 9\n",
      "se â†’ 10\n",
      "\n",
      "Ejemplo de tokenizaciÃ³n:\n",
      "OraciÃ³n: source wikisource librodot com sentido y sensibilidad jane austen capitulo i\n",
      "Secuencia: [8617, 15766, 15767, 11038, 447, 4, 2642, 204, 15768, 3947, 6156]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token=OOV_TOKEN)  # Token para palabras desconocidas\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Convertir frases a secuencias numÃ©ricas\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# TamaÃ±o del vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 por el padding (Ã­ndice 0)\n",
    "print(f\"Vocabulario: {vocab_size} palabras\")\n",
    "\n",
    "print(\"\\nPrimeras 10 palabras del vocabulario:\")\n",
    "for i, (word, idx) in enumerate(list(tokenizer.word_index.items())[:10]):\n",
    "    print(f\"{word} â†’ {idx}\")\n",
    "\n",
    "print(f\"\\nEjemplo de tokenizaciÃ³n:\")\n",
    "example_sentence = sentences[0]\n",
    "example_sequence = sequences[0]\n",
    "print(f\"OraciÃ³n: {example_sentence}\")\n",
    "print(f\"Secuencia: {example_sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CreaciÃ³n del Conjunto de Entrenamiento (X, Y)\n",
    "\n",
    "Para cada secuencia `[w1, w2, ..., wn]`, se crean pares:\n",
    "- `(w1) â†’ w2`\n",
    "- `(w1, w2) â†’ w3`\n",
    "- `...`\n",
    "- `(w1, ..., wn-1) â†’ wn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de secuencias de entrenamiento generadas: 475334\n",
      "\n",
      "Ejemplos de secuencias X â†’ y:\n",
      "['source'] â†’ wikisource\n",
      "['source', 'wikisource'] â†’ librodot\n",
      "['source', 'wikisource', 'librodot'] â†’ com\n",
      "['source', 'wikisource', 'librodot', 'com'] â†’ sentido\n",
      "['source', 'wikisource', 'librodot', 'com', 'sentido'] â†’ y\n"
     ]
    }
   ],
   "source": [
    "X = []  # Secuencias de entrada: [palabra1], [palabra1, palabra2], ...\n",
    "y = []  # Palabra siguiente (objetivo)\n",
    "\n",
    "for seq in sequences:\n",
    "    for i in range(len(seq) - 1):\n",
    "        X.append(seq[:i+1])    # Desde el inicio hasta la palabra actual\n",
    "        y.append(seq[i+1])     # La siguiente palabra\n",
    "\n",
    "print(f\"Total de secuencias de entrenamiento generadas: {len(X)}\")\n",
    "print(\"\\nEjemplos de secuencias X â†’ y:\")\n",
    "for i in range(5):\n",
    "    # Convertir Ã­ndices a palabras para mostrar\n",
    "    x_words = [tokenizer.index_word.get(idx, '<UNK>') for idx in X[i]]\n",
    "    y_word = tokenizer.index_word.get(y[i], '<UNK>')\n",
    "    print(f\"{x_words} â†’ {y_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Padding y Truncado (MAX_LEN â‰¤ 50)\n",
    "\n",
    "Se Aplica padding a las secuencias y se trunca si la longitud mÃ¡xima excede 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud mÃ¡xima encontrada: 321\n",
      "MAX_LEN aplicado (truncado a 50): 50\n",
      "\n",
      "Forma de X_padded: (475334, 50)\n",
      "Forma de y: (475334,)\n",
      "\n",
      "Ejemplo de X_padded (primeras 3 secuencias):\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0  8617]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   8617 15766]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0  8617\n",
      "  15766 15767]]\n",
      "\n",
      "Ejemplo de y (primeras 10 etiquetas):\n",
      "[15766 15767 11038   447     4  2642   204 15768  3947  6156]\n"
     ]
    }
   ],
   "source": [
    "# Longitud mÃ¡xima de las secuencias (con restricciÃ³n MAX_LEN <= 50)\n",
    "raw_max_length = max([len(seq) for seq in X])\n",
    "MAX_LEN = min(50, raw_max_length)  # Aplicar restricciÃ³n de mÃ¡ximo 50\n",
    "print(f\"Longitud mÃ¡xima encontrada: {raw_max_length}\")\n",
    "print(f\"MAX_LEN aplicado (truncado a 50): {MAX_LEN}\")\n",
    "\n",
    "# Aplicar padding y truncado a las secuencias de entrada\n",
    "X_padded = pad_sequences(X, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating='pre')\n",
    "y = np.array(y)  # Etiquetas: ya es un array 1D\n",
    "\n",
    "print(f\"\\nForma de X_padded: {X_padded.shape}\")  # (nÃºmero de muestras, MAX_LEN)\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(\"\\nEjemplo de X_padded (primeras 3 secuencias):\")\n",
    "print(X_padded[:3])\n",
    "print(\"\\nEjemplo de y (primeras 10 etiquetas):\")\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DivisiÃ³n del Conjunto (Train/Test: 80%/20%)\n",
    "\n",
    "Se dividen los datos en conjuntos de entrenamiento y prueba usando `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DivisiÃ³n del conjunto:\n",
      "X_train: (380267, 50), y_train: (380267,)\n",
      "X_test: (95067, 50), y_test: (95067,)\n",
      "\n",
      "Porcentajes:\n",
      "Entrenamiento: 80.0%\n",
      "Prueba: 20.0%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_padded, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"DivisiÃ³n del conjunto:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"\\nPorcentajes:\")\n",
    "print(f\"Entrenamiento: {len(X_train)/(len(X_train)+len(X_test))*100:.1f}%\")\n",
    "print(f\"Prueba: {len(X_test)/(len(X_train)+len(X_test))*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ConstrucciÃ³n del Modelo LSTM/BiLSTM\n",
    "\n",
    "Se crea el modelo con arquitectura configurable (LSTM o BiLSTM) y se compila con `sparse_categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yenreh/anaconda3/envs/py310ia/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758846730.092595  150397 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9683 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Crea un modelo de lenguaje usando BiLSTM o LSTM segÃºn configuraciÃ³n.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): TamaÃ±o del vocabulario\n",
    "        max_length (int): Longitud mÃ¡xima de las secuencias\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Modelo compilado\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length, name='embedding'),\n",
    "        # Usar BiLSTM si estÃ¡ habilitado, sino LSTM unidireccional\n",
    "        Bidirectional(LSTM(LSTM_UNITS, name='lstm'), name='bidirectional_lstm') if USE_BIDIRECTIONAL \n",
    "        else LSTM(LSTM_UNITS, name='lstm'),\n",
    "        Dense(DENSE_UNITS, activation='relu', name='dense_hidden'),\n",
    "        Dense(vocab_size, activation='softmax', name='output')  # Probabilidades para cada palabra\n",
    "    ])\n",
    "    \n",
    "    # Configurar optimizador con tasa de aprendizaje personalizada\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    # Usar sparse_categorical_crossentropy (NO necesita one-hot encoding)\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy', 'sparse_top_k_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(vocab_size, MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento del Modelo con Early Stopping\n",
    "\n",
    "Se entrena el modelo con Early Stopping para prevenir overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando el modelo LSTM...\n",
      "Arquitectura: 300D embedding â†’ LSTM(128) â†’ Dense(128) â†’ Softmax(30415)\n",
      "ParÃ¡metros de entrenamiento: 30 epochs, batch_size=32, lr=0.001\n",
      "Early Stopping habilitado: val_loss con patience=5\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 19:32:11.808522: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9507/9507\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 18ms/step - accuracy: 0.0777 - loss: 6.6866 - sparse_top_k_categorical_accuracy: 0.2336 - val_accuracy: 0.0947 - val_loss: 6.4900 - val_sparse_top_k_categorical_accuracy: 0.2523\n",
      "Epoch 2/30\n",
      "\u001b[1m9507/9507\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 18ms/step - accuracy: 0.1114 - loss: 6.0687 - sparse_top_k_categorical_accuracy: 0.2697 - val_accuracy: 0.1135 - val_loss: 6.3238 - val_sparse_top_k_categorical_accuracy: 0.2730\n",
      "Epoch 3/30\n",
      "\u001b[1m9507/9507\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 18ms/step - accuracy: 0.1302 - loss: 5.7086 - sparse_top_k_categorical_accuracy: 0.2893 - val_accuracy: 0.1186 - val_loss: 6.3299 - val_sparse_top_k_categorical_accuracy: 0.2813\n",
      "Epoch 4/30\n",
      "\u001b[1m9505/9507\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1451 - loss: 5.4439 - sparse_top_k_categorical_accuracy: 0.3045"
     ]
    }
   ],
   "source": [
    "# Configurar Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     # MÃ©trica a monitorear\n",
    "    patience=5,             # NÃºmero de Ã©pocas sin mejora antes de parar\n",
    "    restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "    verbose=1               # Mostrar informaciÃ³n cuando se pare\n",
    ")\n",
    "\n",
    "print(f\"Entrenando el modelo {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}...\")\n",
    "print(f\"Arquitectura: {EMBEDDING_DIM}D embedding â†’ {'Bi-' if USE_BIDIRECTIONAL else ''}LSTM({LSTM_UNITS}) â†’ Dense({DENSE_UNITS}) â†’ Softmax({vocab_size})\")\n",
    "print(f\"ParÃ¡metros de entrenamiento: {EPOCHS} epochs, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(\"Early Stopping habilitado: val_loss con patience=5\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,  # Usar conjuntos de entrenamiento divididos\n",
    "    epochs=EPOCHS,\n",
    "    verbose=VERBOSE_TRAINING,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,  # Usar datos de entrenamiento para validaciÃ³n\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping]  # Agregar Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. CÃ¡lculo de la Perplejidad\n",
    "\n",
    "Se immplementan las funciones para calcular la perplejidad.\n",
    "\n",
    "**FÃ³rmula:** `perplexity = exp(-average_log_likelihood)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, X_test, y_test, batch_size=32):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad del modelo en el conjunto de prueba usando CPU.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        X_test: Conjunto de prueba de entrada\n",
    "        y_test: Conjunto de prueba de salida (etiquetas verdaderas)\n",
    "        batch_size: TamaÃ±o de batch para predict\n",
    "    \n",
    "    Returns:\n",
    "        float: Valor de perplejidad\n",
    "    \"\"\"\n",
    "    # Forzar CPU para evitar problemas de memoria en GPU\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        predictions = model.predict(X_test, verbose=0, batch_size=batch_size)\n",
    "\n",
    "    # Calcular log-likelihood promedio\n",
    "    log_likelihoods = []\n",
    "    for i in range(len(y_test)):\n",
    "        true_word_idx = y_test[i]\n",
    "        predicted_prob = predictions[i][true_word_idx]\n",
    "        predicted_prob = max(predicted_prob, 1e-10)  # evitar log(0)\n",
    "        log_likelihoods.append(np.log(predicted_prob))\n",
    "\n",
    "    average_log_likelihood = np.mean(log_likelihoods)\n",
    "    perplexity = np.exp(-average_log_likelihood)\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "def interpret_perplexity(perplexity_value):\n",
    "    \"\"\"\n",
    "    Interpreta el valor de perplejidad segÃºn rangos comunes.\n",
    "    \n",
    "    Args:\n",
    "        perplexity_value (float): Valor de perplejidad calculado\n",
    "        \n",
    "    Returns:\n",
    "        str: InterpretaciÃ³n del valor\n",
    "    \"\"\"\n",
    "    if perplexity_value < 10:\n",
    "        return \"Excelente\"\n",
    "    elif perplexity_value < 50:\n",
    "        return \"Muy bueno\"\n",
    "    elif perplexity_value < 100:\n",
    "        return \"Bueno\"\n",
    "    elif perplexity_value < 200:\n",
    "        return \"Aceptable\"\n",
    "    elif perplexity_value < 500:\n",
    "        return \"Regular\"\n",
    "    else:\n",
    "        return \"Pobre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment_results(history, model, perplexity, vocab_size, X_train, X_test, MAX_LEN):\n",
    "    \"\"\"\n",
    "    Guarda los resultados del experimento en un archivo JSON para llevar historial.\n",
    "    \n",
    "    Args:\n",
    "        history: Historial del entrenamiento\n",
    "        model: Modelo entrenado\n",
    "        perplexity: Valor de perplejidad calculado\n",
    "        vocab_size: TamaÃ±o del vocabulario\n",
    "        X_train, X_test: Conjuntos de datos para obtener tamaÃ±os\n",
    "        MAX_LEN: Longitud mÃ¡xima de secuencia\n",
    "    \"\"\"\n",
    "    # Preparar datos del experimento\n",
    "    experiment_data = {\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"configuration\": {\n",
    "            \"model_type\": \"BiLSTM\" if USE_BIDIRECTIONAL else \"LSTM\",\n",
    "            \"embedding_dim\": EMBEDDING_DIM,\n",
    "            \"lstm_units\": LSTM_UNITS,\n",
    "            \"dense_units\": DENSE_UNITS,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"dataset_take\": DATASET_TAKE,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"padding_type\": PADDING_TYPE,\n",
    "            \"min_words_per_sentence\": MIN_WORDS_PER_SENTENCE,\n",
    "            \"gpu_used\": len(tf.config.list_physical_devices('GPU')) > 0 \n",
    "        },\n",
    "        \"dataset_info\": {\n",
    "            \"vocab_size\": int(vocab_size),\n",
    "            \"train_samples\": int(len(X_train)),\n",
    "            \"test_samples\": int(len(X_test)),\n",
    "            \"dataset_name\": DATASET_NAME\n",
    "        },\n",
    "        \"training_results\": {\n",
    "            \"final_train_loss\": float(history.history['loss'][-1]),\n",
    "            \"final_train_accuracy\": float(history.history['accuracy'][-1]),\n",
    "            \"final_val_loss\": float(history.history.get('val_loss', [-1])[-1]) if 'val_loss' in history.history else None,\n",
    "            \"final_val_accuracy\": float(history.history.get('val_accuracy', [-1])[-1]) if 'val_accuracy' in history.history else None,\n",
    "            \"epochs_trained\": len(history.history['loss']),\n",
    "            \"model_parameters\": int(model.count_params())\n",
    "        },\n",
    "        \"evaluation_metrics\": {\n",
    "            \"perplexity\": float(perplexity),\n",
    "            \"perplexity_interpretation\": interpret_perplexity(perplexity)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Nombre del archivo de historial\n",
    "    results_file = \"experiment_history.json\"\n",
    "    \n",
    "    # Cargar historial existente o crear uno nuevo\n",
    "    if os.path.exists(results_file):\n",
    "        try:\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                history_data = json.load(f)\n",
    "        except:\n",
    "            history_data = {\"experiments\": []}\n",
    "    else:\n",
    "        history_data = {\"experiments\": []}\n",
    "    \n",
    "    # Agregar nuevo experimento\n",
    "    experiment_data[\"experiment_id\"] = len(history_data[\"experiments\"]) + 1\n",
    "    history_data[\"experiments\"].append(experiment_data)\n",
    "    \n",
    "    # Guardar archivo actualizado\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(history_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Experimento #{experiment_data['experiment_id']} guardado en {results_file}\")\n",
    "    return results_file\n",
    "\n",
    "print(\"FunciÃ³n de guardado de experimentos definida exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. EvaluaciÃ³n Completa del Modelo\n",
    "\n",
    "Se evalua el rendimiento del modelo incluyendo perplejidad y mÃ©tricas estÃ¡ndar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(history, model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Muestra mÃ©tricas de rendimiento del modelo durante el entrenamiento y evaluaciÃ³n.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== MÃ‰TRICAS DE ENTRENAMIENTO ===\")\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_accuracy = history.history['accuracy'][-1]\n",
    "    \n",
    "    if 'val_loss' in history.history:\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "        print(f\"Loss final: {final_loss:.4f} | Val Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"Accuracy final: {final_accuracy:.4f} | Val Accuracy: {final_val_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(f\"Loss final: {final_loss:.4f}\")\n",
    "        print(f\"Accuracy final: {final_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluar en conjunto de prueba\n",
    "    print(\"\\n=== EVALUACIÃ“N EN CONJUNTO DE PRUEBA ===\")\n",
    "    test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_loss = test_results[0]\n",
    "    test_accuracy = test_results[1]\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Si hay mÃ©tricas adicionales, mostrarlas tambiÃ©n\n",
    "    if len(test_results) > 2:\n",
    "        test_top_k_accuracy = test_results[2]\n",
    "        print(f\"Test Top-K Accuracy: {test_top_k_accuracy:.4f}\")\n",
    "    \n",
    "    # Calcular perplejidad\n",
    "    print(\"\\n=== CÃLCULO DE PERPLEJIDAD ===\")\n",
    "    perplexity = calculate_perplexity(model, X_test, y_test)\n",
    "    interpretation = interpret_perplexity(perplexity)\n",
    "    \n",
    "    print(f\"Perplejidad en conjunto de prueba: {perplexity:.2f}\")\n",
    "    print(f\"InterpretaciÃ³n: {interpretation}\")\n",
    "    \n",
    "    # Tabla de interpretaciÃ³n\n",
    "    print(\"\\n=== TABLA DE INTERPRETACIÃ“N DE PERPLEJIDAD ===\")\n",
    "    print(\"< 10:     Excelente\")\n",
    "    print(\"10-50:    Muy bueno\") \n",
    "    print(\"50-100:   Bueno\")\n",
    "    print(\"100-200:  Aceptable\")\n",
    "    print(\"200-500:  Regular\")\n",
    "    print(\"> 500:    Pobre\")\n",
    "    \n",
    "    # Guardar resultados del experimento automÃ¡ticamente\n",
    "    print(\"\\n=== GUARDANDO RESULTADOS DEL EXPERIMENTO ===\")\n",
    "    try:\n",
    "        save_experiment_results(history, model, perplexity, vocab_size, X_train, X_test, MAX_LEN)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar experimento: {e}\")\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Evaluar rendimiento (incluyendo perplejidad)\n",
    "perplexity = evaluate_model_performance(history, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PredicciÃ³n de la PrÃ³xima Palabra\n",
    "\n",
    "Se implementa la funciÃ³n `predict_next_word` para predecir la siguiente palabra dada una secuencia de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, sentence, max_length, top_k=1):\n",
    "    \"\"\"\n",
    "    Predice la(s) palabra(s) siguiente(s) dada una oraciÃ³n en espaÃ±ol.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizador ajustado\n",
    "        sentence (str): OraciÃ³n de entrada\n",
    "        max_length (int): Longitud mÃ¡xima de secuencia\n",
    "        top_k (int): NÃºmero de predicciones top a retornar\n",
    "    \n",
    "    Returns:\n",
    "        str o list: Palabra predicha (top_k=1) o lista de predicciones (top_k>1)\n",
    "    \"\"\"\n",
    "    # Tokenizar la oraciÃ³n\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "    if len(sequence) == 0:\n",
    "        return \"<no_reconocido>\" if top_k == 1 else [\"<no_reconocido>\"]\n",
    "\n",
    "    # Aplicar padding usando el mismo tipo configurado\n",
    "    sequence_padded = pad_sequences([sequence], maxlen=max_length, padding=PADDING_TYPE, truncating='pre')\n",
    "\n",
    "    # Predecir probabilidades\n",
    "    prediction = model.predict(sequence_padded, verbose=VERBOSE_PREDICTION)\n",
    "    \n",
    "    if top_k == 1:\n",
    "        # Retornar solo la predicciÃ³n mÃ¡s probable\n",
    "        predicted_idx = np.argmax(prediction[0])\n",
    "        predicted_word = tokenizer.index_word.get(predicted_idx, \"<desconocido>\")\n",
    "        return predicted_word\n",
    "    else:\n",
    "        # Retornar las top_k predicciones mÃ¡s probables\n",
    "        top_indices = np.argsort(prediction[0])[-top_k:][::-1]\n",
    "        predictions = []\n",
    "        for idx in top_indices:\n",
    "            word = tokenizer.index_word.get(idx, \"<desconocido>\")\n",
    "            prob = prediction[0][idx]\n",
    "            predictions.append((word, prob))\n",
    "        return predictions\n",
    "\n",
    "def generate_text(model, tokenizer, seed_text, max_length, num_words_to_generate=10):\n",
    "    \"\"\"\n",
    "    Genera texto continuando desde un texto semilla.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizador ajustado\n",
    "        seed_text (str): Texto inicial\n",
    "        max_length (int): Longitud mÃ¡xima de secuencia\n",
    "        num_words_to_generate (int): NÃºmero de palabras a generar\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto generado\n",
    "    \"\"\"\n",
    "    generated_text = seed_text\n",
    "    \n",
    "    for _ in range(num_words_to_generate):\n",
    "        next_word = predict_next_word(model, tokenizer, generated_text, max_length)\n",
    "        if next_word in [\"<no_reconocido>\", \"<desconocido>\"]:\n",
    "            break\n",
    "        generated_text += \" \" + next_word\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"Funciones de predicciÃ³n definidas exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pruebas de PredicciÃ³n de Siguiente Palabra\n",
    "\n",
    "Se prueba el modelo con casos de prueba en espaÃ±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos de prueba para predicciÃ³n de siguiente palabra\n",
    "test_cases = [\n",
    "    \"el gato se sentÃ³ en la\",\n",
    "    \"los estudiantes abrieron sus\",\n",
    "    \"la maestra escribiÃ³ en el\",\n",
    "    \"el niÃ±o jugÃ³ con un\",\n",
    "    \"el pÃ¡jaro volÃ³ sobre el\",\n",
    "    \"el sol se elevÃ³ en el\",\n",
    "    \"la casa tiene una\",\n",
    "    \"me gusta comer\",\n",
    "    \"vamos a la\"\n",
    "]\n",
    "\n",
    "print(f\"=== PREDICCIÃ“N DE SIGUIENTE PALABRA ({'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}) ===\")\n",
    "for sentence in test_cases:\n",
    "    next_word = predict_next_word(model, tokenizer, sentence, MAX_LEN)\n",
    "    print(f\"'{sentence}' â†’ '{next_word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Predicciones Top-K\n",
    "\n",
    "Se muestran las K(3) predicciones mÃ¡s probables para algunos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones top-k para algunos casos\n",
    "print(f\"=== TOP-3 PREDICCIONES ===\")\n",
    "for sentence in test_cases[:3]:\n",
    "    top_predictions = predict_next_word(model, tokenizer, sentence, MAX_LEN, top_k=3)\n",
    "    print(f\"'{sentence}':\")\n",
    "    for i, (word, prob) in enumerate(top_predictions, 1):\n",
    "        print(f\"  {i}. '{word}' (probabilidad: {prob:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. GeneraciÃ³n de Texto\n",
    "\n",
    "Se genera texto continuando desde un texto semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeneraciÃ³n de texto\n",
    "print(f\"=== GENERACIÃ“N DE TEXTO ===\")\n",
    "seed_texts = [\n",
    "    \"el perro\",\n",
    "    \"los estudiantes\",\n",
    "    \"en la casa\"\n",
    "]\n",
    "\n",
    "for seed in seed_texts:\n",
    "    generated = generate_text(model, tokenizer, seed, MAX_LEN, num_words_to_generate=8)\n",
    "    print(f\"Semilla: '{seed}'\")\n",
    "    print(f\"Generado: '{generated}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Resumen Final del Modelo\n",
    "\n",
    "Se muestra un resumen completo de las caracterÃ­sticas y rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RESUMEN DEL MODELO ===\")\n",
    "print(f\"Vocabulario: {vocab_size:,} palabras\")\n",
    "print(f\"Secuencias de entrenamiento: {len(X_train):,}\")\n",
    "print(f\"Secuencias de prueba: {len(X_test):,}\")\n",
    "print(f\"Longitud mÃ¡xima de secuencia (MAX_LEN): {MAX_LEN}\")\n",
    "print(f\"Arquitectura: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"ParÃ¡metros del modelo: {model.count_params():,}\")\n",
    "print(f\"Dataset utilizado: {DATASET_NAME} (primeras {DATASET_TAKE:,} muestras)\")\n",
    "print(f\"Perplejidad final: {perplexity:.2f} ({interpret_perplexity(perplexity)})\")\n",
    "\n",
    "print(\"\\n=== COMPONENTES IMPLEMENTADOS ===\")\n",
    "components = [\n",
    "    \"Carga del Dataset (spanish_billion_words_clean)\",\n",
    "    \"TokenizaciÃ³n y CreaciÃ³n del Vocabulario\", \n",
    "    \"CreaciÃ³n del Conjunto de Entrenamiento (X, Y)\",\n",
    "    \"Padding y Truncado (MAX_LEN â‰¤ 50)\",\n",
    "    \"DivisiÃ³n del Conjunto (Train/Test 80%/20%)\",\n",
    "    \"ConstrucciÃ³n del Modelo LSTM/BiLSTM\",\n",
    "    \"Entrenamiento con Early Stopping\",\n",
    "    \"CÃ¡lculo de la Perplejidad\",\n",
    "    \"PredicciÃ³n de la PrÃ³xima Palabra\",\n",
    "    \"Uso de sparse_categorical_crossentropy\"\n",
    "]\n",
    "\n",
    "for component in components:\n",
    "    print(component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "Este notebook implementa completamente los requerimientos de la Tarea 1:\n",
    "\n",
    "1. **Dataset**: Se utilizÃ³ `spanish_billion_words_clean` de Hugging Face\n",
    "2. **TokenizaciÃ³n**: Se ImplementÃ³ con vocabulario Ãºnico y mapeo palabraâ†”Ã­ndice\n",
    "3. **PreparaciÃ³n de datos**: Secuencias (X,Y) con padding/truncado (MAX_LENâ‰¤50)\n",
    "4. **DivisiÃ³n**: 80% entrenamiento / 20% prueba con `train_test_split`\n",
    "5. **Modelo**: Arquitectura LSTM/BiLSTM configurable con embedding\n",
    "6. **Entrenamiento**: Con Early Stopping y `sparse_categorical_crossentropy`\n",
    "7. **EvaluaciÃ³n**: Perplejidad calculada \n",
    "8. **PredicciÃ³n**: FunciÃ³n `predict_next_word`\n",
    "\n",
    "El modelo puede alternar entre LSTM y BiLSTM simplemente cambiando `USE_BIDIRECTIONAL=True/False` en los parÃ¡metros globales.\n",
    "\n",
    "**ParÃ¡metros recomendados para experimentaciÃ³n:**\n",
    "- EMBEDDING_DIM: 100-300\n",
    "- LSTM_UNITS: 64-128\n",
    "- DENSE_UNITS: 64-128\n",
    "- LEARNING_RATE: 0.001-0.01\n",
    "- BATCH_SIZE: 8-32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Historial de Experimentos\n",
    "\n",
    "Este bloque muestra todos los experimentos realizados, permitiendo comparar diferentes configuraciones y resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_experiment_history():\n",
    "    \"\"\"\n",
    "    Carga y muestra el historial completo de experimentos con comparaciones.\n",
    "    \"\"\"\n",
    "    results_file = \"experiment_history.json\"\n",
    "    \n",
    "    if not os.path.exists(results_file):\n",
    "        print(\"No hay historial de experimentos disponible.\")\n",
    "        print(\"Ejecuta el notebook completo para generar el primer experimento.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'r', encoding='utf-8') as f:\n",
    "            history_data = json.load(f)\n",
    "        \n",
    "        experiments = history_data.get(\"experiments\", [])\n",
    "        \n",
    "        if not experiments:\n",
    "            print(\"No hay experimentos en el historial.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"HISTORIAL DE EXPERIMENTOS ({len(experiments)} experimentos)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Mostrar resumen de todos los experimentos\n",
    "        print(\"\\nğŸ” RESUMEN COMPARATIVO:\")\n",
    "        print(f\"{'ID':<3} {'Modelo':<7} {'Emb':<4} {'LSTM':<5} {'Dense':<6} {'Ã‰pocas':<7} {'Perplejidad':<12} {'Interp.':<12} {'GPU':<12}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for exp in experiments:\n",
    "            config = exp['configuration']\n",
    "            metrics = exp['evaluation_metrics']\n",
    "            training = exp['training_results']\n",
    "            \n",
    "            print(f\"{exp['experiment_id']:<3} \"\n",
    "                  f\"{config['model_type']:<7} \"\n",
    "                  f\"{config['embedding_dim']:<4} \"\n",
    "                  f\"{config['lstm_units']:<5} \"\n",
    "                  f\"{config['dense_units']:<6} \"\n",
    "                  f\"{training['epochs_trained']:<7} \"\n",
    "                  f\"{metrics['perplexity']:<12.2f} \"\n",
    "                  f\"{metrics['perplexity_interpretation']:<12} \"\n",
    "                  f\"{'SÃ­' if config['gpu_used'] else 'No':<12}\"  \n",
    "                )\n",
    "        \n",
    "        # Encontrar mejores y peores experimentos\n",
    "        best_exp = min(experiments, key=lambda x: x['evaluation_metrics']['perplexity'])\n",
    "        worst_exp = max(experiments, key=lambda x: x['evaluation_metrics']['perplexity'])\n",
    "        \n",
    "        print(f\"\\n MEJOR EXPERIMENTO (menor perplejidad):\")\n",
    "        print(f\"   ID #{best_exp['experiment_id']}: {best_exp['configuration']['model_type']} \"\n",
    "              f\"con perplejidad {best_exp['evaluation_metrics']['perplexity']:.2f} \"\n",
    "              f\"({best_exp['evaluation_metrics']['perplexity_interpretation']})\")\n",
    "        \n",
    "        print(f\"\\n PEOR EXPERIMENTO (mayor perplejidad):\")\n",
    "        print(f\"   ID #{worst_exp['experiment_id']}: {worst_exp['configuration']['model_type']} \"\n",
    "              f\"con perplejidad {worst_exp['evaluation_metrics']['perplexity']:.2f} \"\n",
    "              f\"({worst_exp['evaluation_metrics']['perplexity_interpretation']})\")\n",
    "        \n",
    "        # Mostrar estadÃ­sticas generales\n",
    "        all_perplexities = [exp['evaluation_metrics']['perplexity'] for exp in experiments]\n",
    "        avg_perplexity = sum(all_perplexities) / len(all_perplexities)\n",
    "        \n",
    "        print(f\"\\n ESTADÃSTICAS GENERALES:\")\n",
    "        print(f\"   Perplejidad promedio: {avg_perplexity:.2f}\")\n",
    "        print(f\"   Rango de perplejidad: {min(all_perplexities):.2f} - {max(all_perplexities):.2f}\")\n",
    "        \n",
    "        # Contar modelos por tipo\n",
    "        lstm_count = sum(1 for exp in experiments if exp['configuration']['model_type'] == 'LSTM')\n",
    "        bilstm_count = sum(1 for exp in experiments if exp['configuration']['model_type'] == 'BiLSTM')\n",
    "        \n",
    "        print(f\"   Experimentos LSTM: {lstm_count}\")\n",
    "        print(f\"   Experimentos BiLSTM: {bilstm_count}\")\n",
    "        \n",
    "        # Mostrar detalles del Ãºltimo experimento\n",
    "        if experiments:\n",
    "            last_exp = experiments[-1]\n",
    "            print(f\"\\n ÃšLTIMO EXPERIMENTO (ID #{last_exp['experiment_id']}):\")\n",
    "            # print(f\"   Fecha: {datetime.datetime.fromisoformat(last_exp['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"   Modelo: {last_exp['configuration']['model_type']}\")\n",
    "            print(f\"   ConfiguraciÃ³n: Emb={last_exp['configuration']['embedding_dim']}, \"\n",
    "                  f\"LSTM={last_exp['configuration']['lstm_units']}, \"\n",
    "                  f\"Dense={last_exp['configuration']['dense_units']}\")\n",
    "            print(f\"   Entrenamiento: {last_exp['training_results']['epochs_trained']} Ã©pocas, \"\n",
    "                  f\"batch_size={last_exp['configuration']['batch_size']}\")\n",
    "            print(f\"   Resultados: Perplejidad={last_exp['evaluation_metrics']['perplexity']:.2f} \"\n",
    "                  f\"({last_exp['evaluation_metrics']['perplexity_interpretation']})\")\n",
    "            print(f\"   Accuracy final: {last_exp['training_results']['final_train_accuracy']:.4f}\")\n",
    "            if last_exp['training_results']['final_val_accuracy']:\n",
    "                print(f\"   Val Accuracy: {last_exp['training_results']['final_val_accuracy']:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error al cargar historial: {e}\")\n",
    "\n",
    "def show_experiment_details(experiment_id):\n",
    "    \"\"\"\n",
    "    Muestra detalles completos de un experimento especÃ­fico.\n",
    "    \n",
    "    Args:\n",
    "        experiment_id (int): ID del experimento a mostrar\n",
    "    \"\"\"\n",
    "    results_file = \"experiment_history.json\"\n",
    "    \n",
    "    if not os.path.exists(results_file):\n",
    "        print(\"No hay historial de experimentos disponible.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'r', encoding='utf-8') as f:\n",
    "            history_data = json.load(f)\n",
    "        \n",
    "        experiments = history_data.get(\"experiments\", [])\n",
    "        experiment = next((exp for exp in experiments if exp['experiment_id'] == experiment_id), None)\n",
    "        \n",
    "        if not experiment:\n",
    "            print(f\"No se encontrÃ³ experimento con ID #{experiment_id}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"DETALLES DEL EXPERIMENTO #{experiment_id}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        #timestamp = datetime.datetime.fromisoformat(experiment['timestamp'])\n",
    "        #print(f\"Fecha: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        config = experiment['configuration']\n",
    "        print(f\"\\n CONFIGURACIÃ“N:\")\n",
    "        print(f\"   Modelo: {config['model_type']}\")\n",
    "        print(f\"   Embedding: {config['embedding_dim']} dimensiones\")\n",
    "        print(f\"   LSTM Units: {config['lstm_units']}\")\n",
    "        print(f\"   Dense Units: {config['dense_units']}\")\n",
    "        print(f\"   Ã‰pocas configuradas: {config['epochs']}\")\n",
    "        print(f\"   Batch Size: {config['batch_size']}\")\n",
    "        print(f\"   Learning Rate: {config['learning_rate']}\")\n",
    "        print(f\"   Dataset samples: {config['dataset_take']:,}\")\n",
    "        print(f\"   MAX_LEN: {config['max_len']}\")\n",
    "        \n",
    "        dataset = experiment['dataset_info']\n",
    "        print(f\"\\n DATOS:\")\n",
    "        print(f\"   Vocabulario: {dataset['vocab_size']:,} palabras\")\n",
    "        print(f\"   Entrenamiento: {dataset['train_samples']:,} muestras\")\n",
    "        print(f\"   Prueba: {dataset['test_samples']:,} muestras\")\n",
    "        \n",
    "        training = experiment['training_results']\n",
    "        print(f\"\\n ENTRENAMIENTO:\")\n",
    "        print(f\"   Ã‰pocas entrenadas: {training['epochs_trained']}\")\n",
    "        print(f\"   Loss final: {training['final_train_loss']:.4f}\")\n",
    "        print(f\"   Accuracy final: {training['final_train_accuracy']:.4f}\")\n",
    "        if training['final_val_loss']:\n",
    "            print(f\"   Validation Loss: {training['final_val_loss']:.4f}\")\n",
    "            print(f\"   Validation Accuracy: {training['final_val_accuracy']:.4f}\")\n",
    "        print(f\"   ParÃ¡metros del modelo: {training['model_parameters']:,}\")\n",
    "        \n",
    "        metrics = experiment['evaluation_metrics']\n",
    "        print(f\"\\n EVALUACIÃ“N:\")\n",
    "        print(f\"   Perplejidad: {metrics['perplexity']:.2f}\")\n",
    "        print(f\"   InterpretaciÃ³n: {metrics['perplexity_interpretation']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al mostrar detalles: {e}\")\n",
    "\n",
    "# Cargar y mostrar historial\n",
    "load_and_display_experiment_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
