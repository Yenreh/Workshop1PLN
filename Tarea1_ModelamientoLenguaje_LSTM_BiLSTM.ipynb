{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 1: Aplicación de RNNs al Modelamiento de Lenguaje en Español (LSTM/BiLSTM)\n",
    "\n",
    "**Curso:** PLN\n",
    "**Objetivo:** Experimentar con modelos de Redes Neuronales Recurrentes (RNNs), específicamente LSTM y BiLSTM, para el modelado del lenguaje en español. \n",
    "**Autor:** Herney Eduardo Quintero Trochez  \n",
    "**Fecha:** 2025  \n",
    "**Universidad:** Universidad Del Valle  \n",
    "**Curso:** Procesamiento de Lenguaje Natural (PLN) - Taller 1\n",
    "## Componentes implementados:\n",
    "1. Carga del Dataset (`spanish_billion_words_clean`)\n",
    "2. Tokenización y Creación del Vocabulario\n",
    "3. Creación del Conjunto de Entrenamiento (X, Y)\n",
    "4. Padding y Truncado (MAX_LEN ≤ 50)\n",
    "5. División del Conjunto (Train/Test 80%/20%)\n",
    "6. Construcción del Modelo LSTM/BiLSTM\n",
    "7. Entrenamiento con Early Stopping\n",
    "8. Cálculo de la Perplejidad\n",
    "9. Predicción de la Próxima Palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuración de Parámetros Globales\n",
    "\n",
    "Esta sección permite modificar fácilmente todos los parámetros del modelo para experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_NAME = \"jhonparra18/spanish_billion_words_clean\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "DATASET_STREAMING = True\n",
    "DATASET_TAKE = 5000  # Número de ejemplos a tomar del dataset\n",
    "MIN_WORDS_PER_SENTENCE = 4  # Filtro: oraciones con al menos N palabras\n",
    "OOV_TOKEN = \"<OOV>\"  # Token para palabras fuera del vocabulario\n",
    "\n",
    "# Model architecture parameters\n",
    "EMBEDDING_DIM = 300  # Dimensión del embedding de palabras\n",
    "LSTM_UNITS = 128 # Número de unidades en las capas LSTM\n",
    "DENSE_UNITS = 128  # Número de unidades en la capa densa intermedia\n",
    "USE_BIDIRECTIONAL = False  # Usar BiLSTM en lugar de LSTM unidireccional\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 30  # Número de épocas de entrenamiento\n",
    "BATCH_SIZE = 512  # Tamaño del batch\n",
    "VALIDATION_SPLIT = 0.2  # Porcentaje de datos para validación\n",
    "LEARNING_RATE = 0.001  # Tasa de aprendizaje\n",
    "PATIENCE = 15  # Paciencia para early stopping\n",
    "\n",
    "# Sequence processing\n",
    "PADDING_TYPE = 'pre'  # Tipo de padding: 'pre' o 'post'\n",
    "\n",
    "# Output parameters\n",
    "VERBOSE_TRAINING = 1  # Nivel de verbose durante entrenamiento\n",
    "VERBOSE_PREDICTION = 0  # Nivel de verbose durante predicción\n",
    "\n",
    "print(f\"Configuración cargada:\")\n",
    "print(f\"- Modelo: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"- Dataset: {DATASET_TAKE:,} muestras de {DATASET_NAME}\")\n",
    "print(f\"- Arquitectura: {EMBEDDING_DIM}D embedding → LSTM({LSTM_UNITS}) → Dense({DENSE_UNITS})\")\n",
    "print(f\"- Entrenamiento: {EPOCHS} epochs, batch_size={BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías\n",
    "\n",
    "Se importan todas las librerías necesarias para el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "print(\"Librerías importadas exitosamente!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU detectada: {gpus}\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No se detectó GPU, usando CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset\n",
    "\n",
    "Se carga el dataset `spanish_billion_words_clean` de Hugging Face y se filtran oraciones muy cortas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cargando dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT, streaming=DATASET_STREAMING).take(DATASET_TAKE)\n",
    "sentences = [example['text'] for example in dataset if len(example['text'].split()) >= MIN_WORDS_PER_SENTENCE]\n",
    "\n",
    "print(f\"Total de oraciones cargadas: {len(sentences)}\")\n",
    "print(\"\\nEjemplos de oraciones:\")\n",
    "for i, sentence in enumerate(sentences[:5]):\n",
    "    print(f\"{i+1}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenización y Creación del Vocabulario\n",
    "\n",
    "Se Tokenizan las oraciones y se crea el vocabulario con mapeo palabra→índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=OOV_TOKEN)  # Token para palabras desconocidas\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Convertir frases a secuencias numéricas\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Tamaño del vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 por el padding (índice 0)\n",
    "print(f\"Vocabulario: {vocab_size} palabras\")\n",
    "\n",
    "print(\"\\nPrimeras 10 palabras del vocabulario:\")\n",
    "for i, (word, idx) in enumerate(list(tokenizer.word_index.items())[:10]):\n",
    "    print(f\"{word} → {idx}\")\n",
    "\n",
    "print(f\"\\nEjemplo de tokenización:\")\n",
    "example_sentence = sentences[0]\n",
    "example_sequence = sequences[0]\n",
    "print(f\"Oración: {example_sentence}\")\n",
    "print(f\"Secuencia: {example_sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creación del Conjunto de Entrenamiento (X, Y)\n",
    "\n",
    "Para cada secuencia `[w1, w2, ..., wn]`, se crean pares:\n",
    "- `(w1) → w2`\n",
    "- `(w1, w2) → w3`\n",
    "- `...`\n",
    "- `(w1, ..., wn-1) → wn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []  # Secuencias de entrada: [palabra1], [palabra1, palabra2], ...\n",
    "y = []  # Palabra siguiente (objetivo)\n",
    "\n",
    "for seq in sequences:\n",
    "    for i in range(len(seq) - 1):\n",
    "        X.append(seq[:i+1])    # Desde el inicio hasta la palabra actual\n",
    "        y.append(seq[i+1])     # La siguiente palabra\n",
    "\n",
    "print(f\"Total de secuencias de entrenamiento generadas: {len(X)}\")\n",
    "print(\"\\nEjemplos de secuencias X → y:\")\n",
    "for i in range(5):\n",
    "    # Convertir índices a palabras para mostrar\n",
    "    x_words = [tokenizer.index_word.get(idx, '<UNK>') for idx in X[i]]\n",
    "    y_word = tokenizer.index_word.get(y[i], '<UNK>')\n",
    "    print(f\"{x_words} → {y_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Padding y Truncado (MAX_LEN ≤ 50)\n",
    "\n",
    "Se Aplica padding a las secuencias y se trunca si la longitud máxima excede 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longitud máxima de las secuencias (con restricción MAX_LEN <= 50)\n",
    "raw_max_length = max([len(seq) for seq in X])\n",
    "MAX_LEN = min(50, raw_max_length)  # Aplicar restricción de máximo 50\n",
    "print(f\"Longitud máxima encontrada: {raw_max_length}\")\n",
    "print(f\"MAX_LEN aplicado (truncado a 50): {MAX_LEN}\")\n",
    "\n",
    "# Aplicar padding y truncado a las secuencias de entrada\n",
    "X_padded = pad_sequences(X, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating='pre')\n",
    "y = np.array(y)  # Etiquetas: ya es un array 1D\n",
    "\n",
    "print(f\"\\nForma de X_padded: {X_padded.shape}\")  # (número de muestras, MAX_LEN)\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(\"\\nEjemplo de X_padded (primeras 3 secuencias):\")\n",
    "print(X_padded[:3])\n",
    "print(\"\\nEjemplo de y (primeras 10 etiquetas):\")\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. División del Conjunto (Train/Test: 80%/20%)\n",
    "\n",
    "Se dividen los datos en conjuntos de entrenamiento y prueba usando `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_padded, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"División del conjunto:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"\\nPorcentajes:\")\n",
    "print(f\"Entrenamiento: {len(X_train)/(len(X_train)+len(X_test))*100:.1f}%\")\n",
    "print(f\"Prueba: {len(X_test)/(len(X_train)+len(X_test))*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Construcción del Modelo LSTM/BiLSTM\n",
    "\n",
    "Se crea el modelo con arquitectura configurable (LSTM o BiLSTM) y se compila con `sparse_categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    Crea un modelo de lenguaje usando BiLSTM o LSTM según configuración.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Tamaño del vocabulario\n",
    "        max_length (int): Longitud máxima de las secuencias\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Modelo compilado\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length, name='embedding'),\n",
    "        # Usar BiLSTM si está habilitado, sino LSTM unidireccional\n",
    "        Bidirectional(LSTM(LSTM_UNITS, name='lstm'), name='bidirectional_lstm') if USE_BIDIRECTIONAL \n",
    "        else LSTM(LSTM_UNITS, name='lstm'),\n",
    "        Dense(DENSE_UNITS, activation='relu', name='dense_hidden'),\n",
    "        Dense(vocab_size, activation='softmax', name='output')  # Probabilidades para cada palabra\n",
    "    ])\n",
    "    \n",
    "    # Configurar optimizador con tasa de aprendizaje personalizada\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    # Usar sparse_categorical_crossentropy (NO necesita one-hot encoding)\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy', 'sparse_top_k_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(vocab_size, MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento del Modelo con Early Stopping\n",
    "\n",
    "Se entrena el modelo con Early Stopping para prevenir overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     # Métrica a monitorear\n",
    "    patience=PATIENCE,             # Número de épocas sin mejora antes de parar\n",
    "    restore_best_weights=True,  # Restaurar los mejores pesos\n",
    "    verbose=1               # Mostrar información cuando se pare\n",
    ")\n",
    "\n",
    "print(f\"Entrenando el modelo {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}...\")\n",
    "print(f\"Arquitectura: {EMBEDDING_DIM}D embedding → {'Bi-' if USE_BIDIRECTIONAL else ''}LSTM({LSTM_UNITS}) → Dense({DENSE_UNITS}) → Softmax({vocab_size})\")\n",
    "print(f\"Parámetros de entrenamiento: {EPOCHS} epochs, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(f\"Early Stopping habilitado: val_loss con patience={PATIENCE}\")\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_name = (\n",
    "    f\"{'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\"\n",
    "    f\"_emb{EMBEDDING_DIM}\"\n",
    "    f\"_take{DATASET_TAKE}\"\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"models/{model_name}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "    \n",
    "history = model.fit(\n",
    "    X_train, y_train,  # Usar conjuntos de entrenamiento divididos\n",
    "    epochs=EPOCHS,\n",
    "    verbose=VERBOSE_TRAINING,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,  # Usar datos de entrenamiento para validación\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping, checkpoint]  # Agregar Early Stopping y ModelCheckpoint\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cálculo de la Perplejidad\n",
    "\n",
    "Se immplementan las funciones para calcular la perplejidad.\n",
    "\n",
    "**Fórmula:** `perplexity = exp(-average_log_likelihood)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, X_test, y_test, batch_size=32):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad del modelo en el conjunto de prueba de forma segura,\n",
    "    procesando en lotes pequeños para evitar OOM en GPU/CPU.\n",
    "    \"\"\"\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    losses = []\n",
    "\n",
    "    # Procesar el conjunto de prueba en mini-lotes\n",
    "    num_samples = len(X_test)\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = start + batch_size\n",
    "        X_batch = X_test[start:end]\n",
    "        y_batch = y_test[start:end]\n",
    "\n",
    "        preds = model(X_batch, training=False)  # no usar .predict (consume mucha RAM)\n",
    "        batch_loss = loss_fn(y_batch, preds).numpy()\n",
    "        losses.append(batch_loss)\n",
    "\n",
    "    mean_loss = np.mean(losses)\n",
    "    perplexity = float(np.exp(mean_loss))  # PPL = exp(loss promedio)\n",
    "    return perplexity\n",
    "\n",
    "def interpret_perplexity(perplexity_value):\n",
    "    \"\"\"\n",
    "    Interpreta el valor de perplejidad según rangos comunes.\n",
    "    \n",
    "    Args:\n",
    "        perplexity_value (float): Valor de perplejidad calculado\n",
    "        \n",
    "    Returns:\n",
    "        str: Interpretación del valor\n",
    "    \"\"\"\n",
    "    if perplexity_value < 10:\n",
    "        return \"Excelente\"\n",
    "    elif perplexity_value < 50:\n",
    "        return \"Muy bueno\"\n",
    "    elif perplexity_value < 100:\n",
    "        return \"Bueno\"\n",
    "    elif perplexity_value < 200:\n",
    "        return \"Aceptable\"\n",
    "    elif perplexity_value < 500:\n",
    "        return \"Regular\"\n",
    "    else:\n",
    "        return \"Pobre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment_results(history, model, perplexity, vocab_size, X_train, X_test, MAX_LEN):\n",
    "    \"\"\"\n",
    "    Guarda los resultados del experimento en un archivo JSON para llevar historial.\n",
    "    \n",
    "    Args:\n",
    "        history: Historial del entrenamiento\n",
    "        model: Modelo entrenado\n",
    "        perplexity: Valor de perplejidad calculado\n",
    "        vocab_size: Tamaño del vocabulario\n",
    "        X_train, X_test: Conjuntos de datos para obtener tamaños\n",
    "        MAX_LEN: Longitud máxima de secuencia\n",
    "    \"\"\"\n",
    "    # Preparar datos del experimento\n",
    "    experiment_data = {\n",
    "        \"configuration\": {\n",
    "            \"model_type\": \"BiLSTM\" if USE_BIDIRECTIONAL else \"LSTM\",\n",
    "            \"embedding_dim\": EMBEDDING_DIM,\n",
    "            \"lstm_units\": LSTM_UNITS,\n",
    "            \"dense_units\": DENSE_UNITS,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"dataset_take\": DATASET_TAKE,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"padding_type\": PADDING_TYPE,\n",
    "            \"patience\": PATIENCE,\n",
    "            \"min_words_per_sentence\": MIN_WORDS_PER_SENTENCE,\n",
    "            \"gpu_used\": len(tf.config.list_physical_devices('GPU')) > 0\n",
    "        },\n",
    "        \"dataset_info\": {\n",
    "            \"vocab_size\": int(vocab_size),\n",
    "            \"train_samples\": int(len(X_train)),\n",
    "            \"test_samples\": int(len(X_test)),\n",
    "            \"dataset_name\": DATASET_NAME\n",
    "        },\n",
    "        \"training_results\": {\n",
    "            \"final_train_loss\": float(history.history['loss'][-1]),\n",
    "            \"final_train_accuracy\": float(history.history['accuracy'][-1]),\n",
    "            \"final_val_loss\": float(history.history.get('val_loss', [-1])[-1]) if 'val_loss' in history.history else None,\n",
    "            \"final_val_accuracy\": float(history.history.get('val_accuracy', [-1])[-1]) if 'val_accuracy' in history.history else None,\n",
    "            \"epochs_trained\": len(history.history['loss']),\n",
    "            \"model_parameters\": int(model.count_params())\n",
    "        },\n",
    "        \"evaluation_metrics\": {\n",
    "            \"perplexity\": float(perplexity),\n",
    "            \"perplexity_interpretation\": interpret_perplexity(perplexity)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Nombre del archivo de historial\n",
    "    results_file = \"experiment_history.json\"\n",
    "    \n",
    "    # Cargar historial existente o crear uno nuevo\n",
    "    if os.path.exists(results_file):\n",
    "        try:\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                history_data = json.load(f)\n",
    "        except:\n",
    "            history_data = {\"experiments\": []}\n",
    "    else:\n",
    "        history_data = {\"experiments\": []}\n",
    "    \n",
    "    # Agregar nuevo experimento\n",
    "    experiment_data[\"experiment_id\"] = len(history_data[\"experiments\"]) + 1\n",
    "    history_data[\"experiments\"].append(experiment_data)\n",
    "    \n",
    "    # Guardar archivo actualizado\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(history_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Experimento #{experiment_data['experiment_id']} guardado en {results_file}\")\n",
    "    return results_file\n",
    "\n",
    "print(\"Función de guardado de experimentos definida exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluación Completa del Modelo\n",
    "\n",
    "Se evalua el rendimiento del modelo incluyendo perplejidad y métricas estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(history, model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Muestra métricas de rendimiento del modelo durante el entrenamiento y evaluación.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== MÉTRICAS DE ENTRENAMIENTO ===\")\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_accuracy = history.history['accuracy'][-1]\n",
    "    \n",
    "    if 'val_loss' in history.history:\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "        print(f\"Loss final: {final_loss:.4f} | Val Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"Accuracy final: {final_accuracy:.4f} | Val Accuracy: {final_val_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(f\"Loss final: {final_loss:.4f}\")\n",
    "        print(f\"Accuracy final: {final_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluar en conjunto de prueba\n",
    "    print(\"\\n=== EVALUACIÓN EN CONJUNTO DE PRUEBA ===\")\n",
    "    test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_loss = test_results[0]\n",
    "    test_accuracy = test_results[1]\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Si hay métricas adicionales, mostrarlas también\n",
    "    if len(test_results) > 2:\n",
    "        test_top_k_accuracy = test_results[2]\n",
    "        print(f\"Test Top-K Accuracy: {test_top_k_accuracy:.4f}\")\n",
    "    \n",
    "    # Calcular perplejidad\n",
    "    print(\"\\n=== CÁLCULO DE PERPLEJIDAD ===\")\n",
    "    perplexity = calculate_perplexity(model, X_test, y_test)\n",
    "    interpretation = interpret_perplexity(perplexity)\n",
    "    \n",
    "    print(f\"Perplejidad en conjunto de prueba: {perplexity:.2f}\")\n",
    "    print(f\"Interpretación: {interpretation}\")\n",
    "    \n",
    "    # Tabla de interpretación\n",
    "    print(\"\\n=== TABLA DE INTERPRETACIÓN DE PERPLEJIDAD ===\")\n",
    "    print(\"< 10:     Excelente\")\n",
    "    print(\"10-50:    Muy bueno\") \n",
    "    print(\"50-100:   Bueno\")\n",
    "    print(\"100-200:  Aceptable\")\n",
    "    print(\"200-500:  Regular\")\n",
    "    print(\"> 500:    Pobre\")\n",
    "    \n",
    "    # Guardar resultados del experimento automáticamente\n",
    "    print(\"\\n=== GUARDANDO RESULTADOS DEL EXPERIMENTO ===\")\n",
    "    try:\n",
    "        save_experiment_results(history, model, perplexity, vocab_size, X_train, X_test, MAX_LEN)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar experimento: {e}\")\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Evaluar rendimiento (incluyendo perplejidad)\n",
    "perplexity = evaluate_model_performance(history, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Predicción de la Próxima Palabra\n",
    "\n",
    "Se implementa la función `predict_next_word` para predecir la siguiente palabra dada una secuencia de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, sentence, max_length, top_k=1):\n",
    "    \"\"\"\n",
    "    Predice la(s) palabra(s) siguiente(s) dada una oración en español.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizador ajustado\n",
    "        sentence (str): Oración de entrada\n",
    "        max_length (int): Longitud máxima de secuencia\n",
    "        top_k (int): Número de predicciones top a retornar\n",
    "    \n",
    "    Returns:\n",
    "        str o list: Palabra predicha (top_k=1) o lista de predicciones (top_k>1)\n",
    "    \"\"\"\n",
    "    # Tokenizar la oración\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "    if len(sequence) == 0:\n",
    "        return \"<no_reconocido>\" if top_k == 1 else [\"<no_reconocido>\"]\n",
    "\n",
    "    sequence_padded = pad_sequences([sequence], maxlen=max_length, \n",
    "                                  padding=PADDING_TYPE, truncating='pre')\n",
    "\n",
    "    # Usar model() en lugar de model.predict()\n",
    "    prediction = model(sequence_padded, training=False)\n",
    "    \n",
    "    if top_k == 1:\n",
    "        predicted_idx = int(np.argmax(prediction[0]))\n",
    "        predicted_word = tokenizer.index_word.get(predicted_idx, \"<desconocido>\")\n",
    "        return predicted_word\n",
    "    else:\n",
    "        # Convertir a numpy para manipulación\n",
    "        prediction_array = prediction[0].numpy()\n",
    "        top_indices = np.argsort(prediction_array)[-top_k:][::-1]\n",
    "        \n",
    "        predictions = []\n",
    "        for idx in top_indices:\n",
    "            word = tokenizer.index_word.get(int(idx), \"<desconocido>\")\n",
    "            prob = float(prediction_array[idx])\n",
    "            predictions.append((word, prob))\n",
    "        return predictions\n",
    "\n",
    "def generate_text(model, tokenizer, seed_text, max_length, num_words_to_generate=10):\n",
    "    \"\"\"\n",
    "    Genera texto continuando desde un texto semilla.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizador ajustado\n",
    "        seed_text (str): Texto inicial\n",
    "        max_length (int): Longitud máxima de secuencia\n",
    "        num_words_to_generate (int): Número de palabras a generar\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto generado\n",
    "    \"\"\"\n",
    "    generated_text = seed_text\n",
    "    \n",
    "    for _ in range(num_words_to_generate):\n",
    "        next_word = predict_next_word(model, tokenizer, generated_text, max_length)\n",
    "        if next_word in [\"<no_reconocido>\", \"<desconocido>\"]:\n",
    "            break\n",
    "        generated_text += \" \" + next_word\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"Funciones de predicción definidas exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pruebas de Predicción de Siguiente Palabra\n",
    "\n",
    "Se prueba el modelo con casos de prueba en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos de prueba para predicción de siguiente palabra\n",
    "test_cases = [\n",
    "    \"el gato se sentó en la\",\n",
    "    \"los estudiantes abrieron sus\",\n",
    "    \"la maestra escribió en el\",\n",
    "    \"el niño jugó con un\",\n",
    "    \"el pájaro voló sobre el\",\n",
    "    \"el sol se elevó en el\",\n",
    "    \"la casa tiene una\",\n",
    "    \"me gusta comer\",\n",
    "    \"vamos a la\"\n",
    "]\n",
    "\n",
    "print(f\"=== PREDICCIÓN DE SIGUIENTE PALABRA ({'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}) ===\")\n",
    "for sentence in test_cases:\n",
    "    next_word = predict_next_word(model, tokenizer, sentence, MAX_LEN)\n",
    "    print(f\"'{sentence}' → '{next_word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Predicciones Top-K\n",
    "\n",
    "Se muestran las K(3) predicciones más probables para algunos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones top-k para algunos casos\n",
    "print(f\"=== TOP-3 PREDICCIONES ===\")\n",
    "for sentence in test_cases[:3]:\n",
    "    top_predictions = predict_next_word(model, tokenizer, sentence, MAX_LEN, top_k=3)\n",
    "    print(f\"'{sentence}':\")\n",
    "    for i, (word, prob) in enumerate(top_predictions, 1):\n",
    "        print(f\"  {i}. '{word}' (probabilidad: {prob:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Generación de Texto\n",
    "\n",
    "Se genera texto continuando desde un texto semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de texto\n",
    "print(f\"=== GENERACIÓN DE TEXTO ===\")\n",
    "seed_texts = [\n",
    "    \"el perro\",\n",
    "    \"los estudiantes\",\n",
    "    \"en la casa\"\n",
    "]\n",
    "\n",
    "for seed in seed_texts:\n",
    "    generated = generate_text(model, tokenizer, seed, MAX_LEN, num_words_to_generate=8)\n",
    "    print(f\"Semilla: '{seed}'\")\n",
    "    print(f\"Generado: '{generated}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Resumen Final del Modelo\n",
    "\n",
    "Se muestra un resumen completo de las características y rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RESUMEN DEL MODELO ===\")\n",
    "print(f\"Vocabulario: {vocab_size:,} palabras\")\n",
    "print(f\"Secuencias de entrenamiento: {len(X_train):,}\")\n",
    "print(f\"Secuencias de prueba: {len(X_test):,}\")\n",
    "print(f\"Longitud máxima de secuencia (MAX_LEN): {MAX_LEN}\")\n",
    "print(f\"Arquitectura: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"Parámetros del modelo: {model.count_params():,}\")\n",
    "print(f\"Dataset utilizado: {DATASET_NAME} (primeras {DATASET_TAKE:,} muestras)\")\n",
    "print(f\"Perplejidad final: {perplexity:.2f} ({interpret_perplexity(perplexity)})\")\n",
    "\n",
    "print(\"\\n=== COMPONENTES IMPLEMENTADOS ===\")\n",
    "components = [\n",
    "    \"Carga del Dataset (spanish_billion_words_clean)\",\n",
    "    \"Tokenización y Creación del Vocabulario\", \n",
    "    \"Creación del Conjunto de Entrenamiento (X, Y)\",\n",
    "    \"Padding y Truncado (MAX_LEN ≤ 50)\",\n",
    "    \"División del Conjunto (Train/Test 80%/20%)\",\n",
    "    \"Construcción del Modelo LSTM/BiLSTM\",\n",
    "    \"Entrenamiento con Early Stopping\",\n",
    "    \"Cálculo de la Perplejidad\",\n",
    "    \"Predicción de la Próxima Palabra\",\n",
    "    \"Uso de sparse_categorical_crossentropy\"\n",
    "]\n",
    "\n",
    "for component in components:\n",
    "    print(component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "Este notebook implementa completamente los requerimientos de la Tarea 1:\n",
    "\n",
    "1. **Dataset**: Se utilizó `spanish_billion_words_clean` de Hugging Face\n",
    "2. **Tokenización**: Se Implementó con vocabulario único y mapeo palabra↔índice\n",
    "3. **Preparación de datos**: Secuencias (X,Y) con padding/truncado (MAX_LEN≤50)\n",
    "4. **División**: 80% entrenamiento / 20% prueba con `train_test_split`\n",
    "5. **Modelo**: Arquitectura LSTM/BiLSTM configurable con embedding\n",
    "6. **Entrenamiento**: Con Early Stopping y `sparse_categorical_crossentropy`\n",
    "7. **Evaluación**: Perplejidad calculada \n",
    "8. **Predicción**: Función `predict_next_word`\n",
    "\n",
    "El modelo puede alternar entre LSTM y BiLSTM simplemente cambiando `USE_BIDIRECTIONAL=True/False` en los parámetros globales.\n",
    "\n",
    "**Parámetros recomendados para experimentación:**\n",
    "- EMBEDDING_DIM: 100-300\n",
    "- LSTM_UNITS: 64-128\n",
    "- DENSE_UNITS: 64-128\n",
    "- LEARNING_RATE: 0.001-0.01\n",
    "- BATCH_SIZE: 8-32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Historial de Experimentos\n",
    "\n",
    "Este bloque muestra todos los experimentos realizados, permitiendo comparar diferentes configuraciones y resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_experiment_history():\n",
    "    \"\"\"\n",
    "    Carga y muestra el historial completo de experimentos con comparaciones.\n",
    "    \"\"\"\n",
    "    results_file = \"experiment_history.json\"\n",
    "    \n",
    "    if not os.path.exists(results_file):\n",
    "        print(\"No hay historial de experimentos disponible.\")\n",
    "        print(\"Ejecuta el notebook completo para generar el primer experimento.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'r', encoding='utf-8') as f:\n",
    "            history_data = json.load(f)\n",
    "        \n",
    "        experiments = history_data.get(\"experiments\", [])\n",
    "        \n",
    "        if not experiments:\n",
    "            print(\"No hay experimentos en el historial.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"HISTORIAL DE EXPERIMENTOS ({len(experiments)} experimentos)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Mostrar resumen de todos los experimentos\n",
    "        print(\"\\n RESUMEN COMPARATIVO:\")\n",
    "        print(f\"{'ID':<3} {'Modelo':<7} {'Sentencias':<10} {'Emb':<4} {'LSTM':<5} {'Dense':<6} {'Épocas':<7} {'Perplejidad':<12} {'Interpretacion':<15} {'GPU':<7}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for exp in experiments:\n",
    "            config = exp['configuration']\n",
    "            metrics = exp['evaluation_metrics']\n",
    "            training = exp['training_results']\n",
    "            \n",
    "            print(f\"{exp['experiment_id']:<3} \"\n",
    "                  f\"{config['model_type']:<7} \"\n",
    "                  f\"{config['dataset_take']:<10} \"\n",
    "                  f\"{config['embedding_dim']:<4} \"\n",
    "                  f\"{config['lstm_units']:<5} \"\n",
    "                  f\"{config['dense_units']:<6} \"\n",
    "                  f\"{training['epochs_trained']:<7} \"\n",
    "                  f\"{metrics['perplexity']:<12.2f} \"\n",
    "                  f\"{metrics['perplexity_interpretation']:<15} \"\n",
    "                  f\"{'Sí' if config['gpu_used'] else 'No':<7}\"  \n",
    "                )\n",
    "        \n",
    "        # Encontrar mejores y peores experimentos\n",
    "        best_exp = min(experiments, key=lambda x: x['evaluation_metrics']['perplexity'])\n",
    "        worst_exp = max(experiments, key=lambda x: x['evaluation_metrics']['perplexity'])\n",
    "        \n",
    "        print(f\"\\n MEJOR EXPERIMENTO (menor perplejidad):\")\n",
    "        print(f\"   ID #{best_exp['experiment_id']}: {best_exp['configuration']['model_type']} \"\n",
    "              f\"con perplejidad {best_exp['evaluation_metrics']['perplexity']:.2f} \"\n",
    "              f\"({best_exp['evaluation_metrics']['perplexity_interpretation']})\")\n",
    "        \n",
    "        print(f\"\\n PEOR EXPERIMENTO (mayor perplejidad):\")\n",
    "        print(f\"   ID #{worst_exp['experiment_id']}: {worst_exp['configuration']['model_type']} \"\n",
    "              f\"con perplejidad {worst_exp['evaluation_metrics']['perplexity']:.2f} \"\n",
    "              f\"({worst_exp['evaluation_metrics']['perplexity_interpretation']})\")\n",
    "        \n",
    "        # Mostrar estadísticas generales\n",
    "        all_perplexities = [exp['evaluation_metrics']['perplexity'] for exp in experiments]\n",
    "        avg_perplexity = sum(all_perplexities) / len(all_perplexities)\n",
    "        \n",
    "        print(f\"\\n ESTADÍSTICAS GENERALES:\")\n",
    "        print(f\"   Perplejidad promedio: {avg_perplexity:.2f}\")\n",
    "        print(f\"   Rango de perplejidad: {min(all_perplexities):.2f} - {max(all_perplexities):.2f}\")\n",
    "        \n",
    "        # Contar modelos por tipo\n",
    "        lstm_count = sum(1 for exp in experiments if exp['configuration']['model_type'] == 'LSTM')\n",
    "        bilstm_count = sum(1 for exp in experiments if exp['configuration']['model_type'] == 'BiLSTM')\n",
    "        \n",
    "        print(f\"   Experimentos LSTM: {lstm_count}\")\n",
    "        print(f\"   Experimentos BiLSTM: {bilstm_count}\")\n",
    "        \n",
    "        # Mostrar detalles del último experimento\n",
    "        if experiments:\n",
    "            last_exp = experiments[-1]\n",
    "            print(f\"\\n ÚLTIMO EXPERIMENTO (ID #{last_exp['experiment_id']}):\")\n",
    "            print(f\"   Modelo: {last_exp['configuration']['model_type']}\")\n",
    "            print(f\"   Configuración: Emb={last_exp['configuration']['embedding_dim']}, \"\n",
    "                  f\"LSTM={last_exp['configuration']['lstm_units']}, \"\n",
    "                  f\"Dense={last_exp['configuration']['dense_units']}\")\n",
    "            print(f\"   Entrenamiento: {last_exp['training_results']['epochs_trained']} épocas, \"\n",
    "                  f\"batch_size={last_exp['configuration']['batch_size']}\")\n",
    "            print(f\"   Resultados: Perplejidad={last_exp['evaluation_metrics']['perplexity']:.2f} \"\n",
    "                  f\"({last_exp['evaluation_metrics']['perplexity_interpretation']})\")\n",
    "            print(f\"   Accuracy final: {last_exp['training_results']['final_train_accuracy']:.4f}\")\n",
    "            if last_exp['training_results']['final_val_accuracy']:\n",
    "                print(f\"   Val Accuracy: {last_exp['training_results']['final_val_accuracy']:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error al cargar historial: {e}\")\n",
    "\n",
    "def show_experiment_details(experiment_id):\n",
    "    \"\"\"\n",
    "    Muestra detalles completos de un experimento específico.\n",
    "    \n",
    "    Args:\n",
    "        experiment_id (int): ID del experimento a mostrar\n",
    "    \"\"\"\n",
    "    results_file = \"experiment_history.json\"\n",
    "    \n",
    "    if not os.path.exists(results_file):\n",
    "        print(\"No hay historial de experimentos disponible.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'r', encoding='utf-8') as f:\n",
    "            history_data = json.load(f)\n",
    "        \n",
    "        experiments = history_data.get(\"experiments\", [])\n",
    "        experiment = next((exp for exp in experiments if exp['experiment_id'] == experiment_id), None)\n",
    "        \n",
    "        if not experiment:\n",
    "            print(f\"No se encontró experimento con ID #{experiment_id}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"DETALLES DEL EXPERIMENTO #{experiment_id}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        config = experiment['configuration']\n",
    "        print(f\"\\n CONFIGURACIÓN:\")\n",
    "        print(f\"   Modelo: {config['model_type']}\")\n",
    "        print(f\"   Embedding: {config['embedding_dim']} dimensiones\")\n",
    "        print(f\"   LSTM Units: {config['lstm_units']}\")\n",
    "        print(f\"   Dense Units: {config['dense_units']}\")\n",
    "        print(f\"   Épocas configuradas: {config['epochs']}\")\n",
    "        print(f\"   Batch Size: {config['batch_size']}\")\n",
    "        print(f\"   Learning Rate: {config['learning_rate']}\")\n",
    "        print(f\"   Dataset samples: {config['dataset_take']:,}\")\n",
    "        print(f\"   MAX_LEN: {config['max_len']}\")\n",
    "        \n",
    "        dataset = experiment['dataset_info']\n",
    "        print(f\"\\n DATOS:\")\n",
    "        print(f\"   Vocabulario: {dataset['vocab_size']:,} palabras\")\n",
    "        print(f\"   Entrenamiento: {dataset['train_samples']:,} muestras\")\n",
    "        print(f\"   Prueba: {dataset['test_samples']:,} muestras\")\n",
    "        \n",
    "        training = experiment['training_results']\n",
    "        print(f\"\\n ENTRENAMIENTO:\")\n",
    "        print(f\"   Épocas entrenadas: {training['epochs_trained']}\")\n",
    "        print(f\"   Loss final: {training['final_train_loss']:.4f}\")\n",
    "        print(f\"   Accuracy final: {training['final_train_accuracy']:.4f}\")\n",
    "        if training['final_val_loss']:\n",
    "            print(f\"   Validation Loss: {training['final_val_loss']:.4f}\")\n",
    "            print(f\"   Validation Accuracy: {training['final_val_accuracy']:.4f}\")\n",
    "        print(f\"   Parámetros del modelo: {training['model_parameters']:,}\")\n",
    "        \n",
    "        metrics = experiment['evaluation_metrics']\n",
    "        print(f\"\\n EVALUACIÓN:\")\n",
    "        print(f\"   Perplejidad: {metrics['perplexity']:.2f}\")\n",
    "        print(f\"   Interpretación: {metrics['perplexity_interpretation']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al mostrar detalles: {e}\")\n",
    "\n",
    "# Cargar y mostrar historial\n",
    "load_and_display_experiment_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
